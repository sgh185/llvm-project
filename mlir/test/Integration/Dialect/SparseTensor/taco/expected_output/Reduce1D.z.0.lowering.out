// -----// IR Dump After LinalgGeneralization (linalg-generalize-named-ops) //----- //
func.func @Reduce1D.z.0.main(%arg0: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = tensor.empty() : tensor<f32>
  %1 = linalg.generic {indexing_maps = [affine_map<() -> ()>, affine_map<() -> ()>], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<f32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%arg0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) outs(%1 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %3 = arith.addf %out, %in : f32
    linalg.yield %3 : f32
  } -> tensor<f32>
  return %2 : tensor<f32>
}

// -----// IR Dump After PreSparsificationRewrite (pre-sparsification-rewrite) //----- //
#map = affine_map<() -> ()>
#map1 = affine_map<(d0) -> (d0)>
#map2 = affine_map<(d0) -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<f32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<f32>
    %2 = linalg.generic {indexing_maps = [#map1, #map2], iterator_types = ["reduction"]} ins(%arg0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) outs(%1 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.addf %out, %in : f32
      linalg.yield %3 : f32
    } -> tensor<f32>
    return %2 : tensor<f32>
  }
}


// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
func.func @Reduce1D.z.0.main(%arg0: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = bufferization.alloc_tensor() : tensor<f32>
  %1 = linalg.generic {indexing_maps = [affine_map<() -> ()>, affine_map<() -> ()>], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<f32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%arg0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) outs(%1 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %3 = arith.addf %out, %in : f32
    linalg.yield %3 : f32
  } -> tensor<f32>
  return %2 : tensor<f32>
}

// -----// IR Dump After SparsificationPass (sparsification) //----- //
#map = affine_map<() -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<f32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<f32>
    %2 = sparse_tensor.positions %arg0 {level = 0 : index} : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>> to memref<?xindex>
    %3 = sparse_tensor.values %arg0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>> to memref<?xf32>
    %4 = bufferization.to_memref %1 : memref<f32>
    %5 = memref.load %4[] : memref<f32>
    %6 = memref.load %2[%c0] : memref<?xindex>
    %7 = memref.load %2[%c1] : memref<?xindex>
    %8 = scf.parallel (%arg1) = (%6) to (%7) step (%c1) init (%5) -> f32 {
      %10 = memref.load %3[%arg1] : memref<?xf32>
      scf.reduce(%10)  : f32 {
      ^bb0(%arg2: f32, %arg3: f32):
        %11 = arith.addf %arg2, %arg3 : f32
        scf.reduce.return %11 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %8, %4[] : memref<f32>
    %9 = bufferization.to_tensor %4 : memref<f32>
    return %9 : tensor<f32>
  }
}


// -----// IR Dump After PostSparsificationRewrite (post-sparsification-rewrite) //----- //
#map = affine_map<() -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<f32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<f32>
    %2 = sparse_tensor.positions %arg0 {level = 0 : index} : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>> to memref<?xindex>
    %3 = sparse_tensor.values %arg0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>> to memref<?xf32>
    %4 = bufferization.to_memref %1 : memref<f32>
    %5 = memref.load %4[] : memref<f32>
    %6 = memref.load %2[%c0] : memref<?xindex>
    %7 = memref.load %2[%c1] : memref<?xindex>
    %8 = scf.parallel (%arg1) = (%6) to (%7) step (%c1) init (%5) -> f32 {
      %10 = memref.load %3[%arg1] : memref<?xf32>
      scf.reduce(%10)  : f32 {
      ^bb0(%arg2: f32, %arg3: f32):
        %11 = arith.addf %arg2, %arg3 : f32
        scf.reduce.return %11 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %8, %4[] : memref<f32>
    %9 = bufferization.to_tensor %4 : memref<f32>
    return %9 : tensor<f32>
  }
}


// -----// IR Dump After SparseTensorCodegen (sparse-tensor-codegen) //----- //
#map = affine_map<() -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<f32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<f32>
    %2 = bufferization.to_memref %1 : memref<f32>
    %3 = memref.load %2[] : memref<f32>
    %4 = memref.load %arg0[%c0] : memref<?xindex>
    %5 = memref.load %arg0[%c1] : memref<?xindex>
    %6 = scf.parallel (%arg4) = (%4) to (%5) step (%c1) init (%3) -> f32 {
      %8 = memref.load %arg2[%arg4] : memref<?xf32>
      scf.reduce(%8)  : f32 {
      ^bb0(%arg5: f32, %arg6: f32):
        %9 = arith.addf %arg5, %arg6 : f32
        scf.reduce.return %9 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %6, %2[] : memref<f32>
    %7 = bufferization.to_tensor %2 : memref<f32>
    return %7 : tensor<f32>
  }
}


// -----// IR Dump After SparseBufferRewrite (sparse-buffer-rewrite) //----- //
#map = affine_map<() -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "compressed" ] }>>) -> tensor<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<f32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<f32>
    %2 = bufferization.to_memref %1 : memref<f32>
    %3 = memref.load %2[] : memref<f32>
    %4 = memref.load %arg0[%c0] : memref<?xindex>
    %5 = memref.load %arg0[%c1] : memref<?xindex>
    %6 = scf.parallel (%arg4) = (%4) to (%5) step (%c1) init (%3) -> f32 {
      %8 = memref.load %arg2[%arg4] : memref<?xf32>
      scf.reduce(%8)  : f32 {
      ^bb0(%arg5: f32, %arg6: f32):
        %9 = arith.addf %arg5, %arg6 : f32
        scf.reduce.return %9 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %6, %2[] : memref<f32>
    %7 = bufferization.to_tensor %2 : memref<f32>
    return %7 : tensor<f32>
  }
}


// -----// IR Dump After StorageSpecifierToLLVM (sparse-storage-specifier-to-llvm) //----- //
#map = affine_map<() -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> tensor<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<f32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%0 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<f32>
    %2 = bufferization.to_memref %1 : memref<f32>
    %3 = memref.load %2[] : memref<f32>
    %4 = memref.load %arg0[%c0] : memref<?xindex>
    %5 = memref.load %arg0[%c1] : memref<?xindex>
    %6 = scf.parallel (%arg4) = (%4) to (%5) step (%c1) init (%3) -> f32 {
      %8 = memref.load %arg2[%arg4] : memref<?xf32>
      scf.reduce(%8)  : f32 {
      ^bb0(%arg5: f32, %arg6: f32):
        %9 = arith.addf %arg5, %arg6 : f32
        scf.reduce.return %9 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %6, %2[] : memref<f32>
    %7 = bufferization.to_tensor %2 : memref<f32>
    return %7 : tensor<f32>
  }
}


// -----// IR Dump After mlir::sparse_tensor::SparsificationAndBufferizationPass () //----- //
#map = affine_map<() -> ()>
module {
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
    linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst : f32) outs(%alloc : memref<f32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %0 = memref.load %alloc[] : memref<f32>
    %1 = memref.load %arg0[%c0] : memref<?xindex>
    %2 = memref.load %arg0[%c1] : memref<?xindex>
    %3 = scf.parallel (%arg4) = (%1) to (%2) step (%c1) init (%0) -> f32 {
      %4 = memref.load %arg2[%arg4] : memref<?xf32>
      scf.reduce(%4)  : f32 {
      ^bb0(%arg5: f32, %arg6: f32):
        %5 = arith.addf %arg5, %arg6 : f32
        scf.reduce.return %5 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %3, %alloc[] : memref<f32>
    return %alloc : memref<f32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
  linalg.generic {indexing_maps = [affine_map<() -> ()>, affine_map<() -> ()>], iterator_types = []} ins(%cst : f32) outs(%alloc : memref<f32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  }
  %0 = memref.load %alloc[] : memref<f32>
  %1 = memref.load %arg0[%c0] : memref<?xindex>
  %2 = memref.load %arg0[%c1] : memref<?xindex>
  %3 = scf.parallel (%arg4) = (%1) to (%2) step (%c1) init (%0) -> f32 {
    %4 = memref.load %arg2[%arg4] : memref<?xf32>
    scf.reduce(%4)  : f32 {
    ^bb0(%arg5: f32, %arg6: f32):
      %5 = arith.addf %arg5, %arg6 : f32
      scf.reduce.return %5 : f32
    }
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  memref.store %3, %alloc[] : memref<f32>
  return %alloc : memref<f32>
}

// -----// IR Dump After FinalizingBufferize (finalizing-bufferize) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
  linalg.generic {indexing_maps = [affine_map<() -> ()>, affine_map<() -> ()>], iterator_types = []} ins(%cst : f32) outs(%alloc : memref<f32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  }
  %0 = memref.load %alloc[] : memref<f32>
  %1 = memref.load %arg0[%c0] : memref<?xindex>
  %2 = memref.load %arg0[%c1] : memref<?xindex>
  %3 = scf.parallel (%arg4) = (%1) to (%2) step (%c1) init (%0) -> f32 {
    %4 = memref.load %arg2[%arg4] : memref<?xf32>
    scf.reduce(%4)  : f32 {
    ^bb0(%arg5: f32, %arg6: f32):
      %5 = arith.addf %arg5, %arg6 : f32
      scf.reduce.return %5 : f32
    }
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  memref.store %3, %alloc[] : memref<f32>
  return %alloc : memref<f32>
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
  memref.store %cst, %alloc[] : memref<f32>
  %0 = memref.load %alloc[] : memref<f32>
  %1 = memref.load %arg0[%c0] : memref<?xindex>
  %2 = memref.load %arg0[%c1] : memref<?xindex>
  %3 = scf.parallel (%arg4) = (%1) to (%2) step (%c1) init (%0) -> f32 {
    %4 = memref.load %arg2[%arg4] : memref<?xf32>
    scf.reduce(%4)  : f32 {
    ^bb0(%arg5: f32, %arg6: f32):
      %5 = arith.addf %arg5, %arg6 : f32
      scf.reduce.return %5 : f32
    }
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  memref.store %3, %alloc[] : memref<f32>
  return %alloc : memref<f32>
}

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
  memref.store %cst, %alloc[] : memref<f32>
  %0 = memref.load %alloc[] : memref<f32>
  %1 = memref.load %arg0[%c0] : memref<?xindex>
  %2 = memref.load %arg0[%c1] : memref<?xindex>
  %3 = scf.parallel (%arg4) = (%1) to (%2) step (%c1) init (%0) -> f32 {
    %4 = memref.load %arg2[%arg4] : memref<?xf32>
    scf.reduce(%4)  : f32 {
    ^bb0(%arg5: f32, %arg6: f32):
      %5 = arith.addf %arg5, %arg6 : f32
      scf.reduce.return %5 : f32
    }
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  memref.store %3, %alloc[] : memref<f32>
  return %alloc : memref<f32>
}

// -----// IR Dump After ConvertSCFToOpenMPPass (convert-scf-to-openmp) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
    memref.store %cst, %alloc[] : memref<f32>
    %0 = memref.load %alloc[] : memref<f32>
    %1 = memref.load %arg0[%c0] : memref<?xindex>
    %2 = memref.load %arg0[%c1] : memref<?xindex>
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
    llvm.store %0, %4 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %4 : !llvm.ptr) for  (%arg4) : index = (%1) to (%2) step (%c1) {
        memref.alloca_scope  {
          %6 = memref.load %arg2[%arg4] : memref<?xf32>
          omp.reduction %6, %4 : f32, !llvm.ptr
        }
        omp.yield
      }
      omp.terminator
    }
    %5 = llvm.load %4 : !llvm.ptr -> f32
    memref.store %5, %alloc[] : memref<f32>
    return %alloc : memref<f32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %0 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
  memref.store %cst, %alloc[] : memref<f32>
  %1 = memref.load %alloc[] : memref<f32>
  %2 = memref.load %arg0[%c0] : memref<?xindex>
  %3 = memref.load %arg0[%c1] : memref<?xindex>
  %4 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
  llvm.store %1, %4 : f32, !llvm.ptr
  omp.parallel   {
    omp.wsloop   reduction(@__scf_reduction -> %4 : !llvm.ptr) for  (%arg4) : index = (%2) to (%3) step (%c1) {
      memref.alloca_scope  {
        %6 = memref.load %arg2[%arg4] : memref<?xf32>
        omp.reduction %6, %4 : f32, !llvm.ptr
      }
      omp.yield
    }
    omp.terminator
  }
  %5 = llvm.load %4 : !llvm.ptr -> f32
  memref.store %5, %alloc[] : memref<f32>
  return %alloc : memref<f32>
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %0 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
  memref.store %cst, %alloc[] : memref<f32>
  %1 = memref.load %alloc[] : memref<f32>
  %2 = memref.load %arg0[%c0] : memref<?xindex>
  %3 = memref.load %arg0[%c1] : memref<?xindex>
  %4 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
  llvm.store %1, %4 : f32, !llvm.ptr
  omp.parallel   {
    omp.wsloop   reduction(@__scf_reduction -> %4 : !llvm.ptr) for  (%arg4) : index = (%2) to (%3) step (%c1) {
      memref.alloca_scope  {
        %6 = memref.load %arg2[%arg4] : memref<?xf32>
        omp.reduction %6, %4 : f32, !llvm.ptr
      }
      omp.yield
    }
    omp.terminator
  }
  %5 = llvm.load %4 : !llvm.ptr -> f32
  memref.store %5, %alloc[] : memref<f32>
  return %alloc : memref<f32>
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
    memref.store %cst, %alloc[] : memref<f32>
    %1 = memref.load %alloc[] : memref<f32>
    %2 = memref.load %arg0[%c0] : memref<?xindex>
    %3 = memref.load %arg0[%c1] : memref<?xindex>
    %4 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
    llvm.store %1, %4 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %4 : !llvm.ptr) for  (%arg4) : index = (%2) to (%3) step (%c1) {
        memref.alloca_scope  {
          %6 = memref.load %arg2[%arg4] : memref<?xf32>
          omp.reduction %6, %4 : f32, !llvm.ptr
        }
        omp.yield
      }
      omp.terminator
    }
    %5 = llvm.load %4 : !llvm.ptr -> f32
    memref.store %5, %alloc[] : memref<f32>
    return %alloc : memref<f32>
  }
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
    memref.store %cst, %alloc[] : memref<f32>
    %1 = memref.load %alloc[] : memref<f32>
    %2 = memref.load %arg0[%c0] : memref<?xindex>
    %3 = memref.load %arg0[%c1] : memref<?xindex>
    %4 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
    llvm.store %1, %4 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %4 : !llvm.ptr) for  (%arg4) : index = (%2) to (%3) step (%c1) {
        memref.alloca_scope  {
          %6 = memref.load %arg2[%arg4] : memref<?xf32>
          omp.reduction %6, %4 : f32, !llvm.ptr
        }
        omp.yield
      }
      omp.terminator
    }
    %5 = llvm.load %4 : !llvm.ptr -> f32
    memref.store %5, %alloc[] : memref<f32>
    return %alloc : memref<f32>
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<f32>
    memref.store %cst, %alloc[] : memref<f32>
    %1 = memref.load %alloc[] : memref<f32>
    %2 = memref.load %arg0[%c0] : memref<?xindex>
    %3 = memref.load %arg0[%c1] : memref<?xindex>
    %4 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
    llvm.store %1, %4 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %4 : !llvm.ptr) for  (%arg4) : index = (%2) to (%3) step (%c1) {
        memref.alloca_scope  {
          %6 = memref.load %arg2[%arg4] : memref<?xf32>
          omp.reduction %6, %4 : f32, !llvm.ptr
        }
        omp.yield
      }
      omp.terminator
    }
    %5 = llvm.load %4 : !llvm.ptr -> f32
    memref.store %5, %alloc[] : memref<f32>
    return %alloc : memref<f32>
  }
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %3 = builtin.unrealized_conversion_cast %c0 : index to i64
    %c1 = arith.constant 1 : index
    %4 = builtin.unrealized_conversion_cast %c1 : index to i64
    %cst = arith.constant 0.000000e+00 : f32
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = llvm.mlir.null : !llvm.ptr
    %7 = llvm.getelementptr %6[%5] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.mlir.constant(64 : index) : i64
    %10 = llvm.add %8, %9  : i64
    %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
    %12 = llvm.ptrtoint %11 : !llvm.ptr to i64
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.sub %9, %13  : i64
    %15 = llvm.add %12, %14  : i64
    %16 = llvm.urem %15, %9  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %20 = llvm.insertvalue %11, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
    %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
    %22 = llvm.mlir.constant(0 : index) : i64
    %23 = llvm.insertvalue %22, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
    %24 = builtin.unrealized_conversion_cast %23 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    %25 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64)> 
    llvm.store %cst, %25 : f32, !llvm.ptr
    %26 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64)> 
    %27 = llvm.load %26 : !llvm.ptr -> f32
    %28 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %29 = llvm.getelementptr %28[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %30 = llvm.load %29 : !llvm.ptr -> i64
    %31 = builtin.unrealized_conversion_cast %30 : i64 to index
    %32 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %33 = llvm.getelementptr %32[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %34 = llvm.load %33 : !llvm.ptr -> i64
    %35 = builtin.unrealized_conversion_cast %34 : i64 to index
    %36 = llvm.alloca %2 x f32 : (i64) -> !llvm.ptr
    llvm.store %27, %36 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %36 : !llvm.ptr) for  (%arg4) : index = (%31) to (%35) step (%c1) {
        %39 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %40 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %41 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %42 = llvm.getelementptr %41[%39] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %43 = llvm.load %42 : !llvm.ptr -> f32
        omp.reduction %43, %36 : f32, !llvm.ptr
        llvm.intr.stackrestore %40 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %37 = llvm.load %36 : !llvm.ptr -> f32
    %38 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64)> 
    llvm.store %37, %38 : f32, !llvm.ptr
    return %24 : memref<f32>
  }
}


// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %1 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %3 = builtin.unrealized_conversion_cast %c0 : index to i64
  %c1 = arith.constant 1 : index
  %4 = builtin.unrealized_conversion_cast %c1 : index to i64
  %cst = arith.constant 0.000000e+00 : f32
  %5 = llvm.mlir.constant(1 : index) : i64
  %6 = llvm.mlir.null : !llvm.ptr
  %7 = llvm.getelementptr %6[1] : (!llvm.ptr) -> !llvm.ptr, f32
  %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
  %9 = llvm.mlir.constant(64 : index) : i64
  %10 = llvm.add %8, %9  : i64
  %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
  %12 = llvm.ptrtoint %11 : !llvm.ptr to i64
  %13 = llvm.mlir.constant(1 : index) : i64
  %14 = llvm.sub %9, %13  : i64
  %15 = llvm.add %12, %14  : i64
  %16 = llvm.urem %15, %9  : i64
  %17 = llvm.sub %15, %16  : i64
  %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
  %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
  %20 = llvm.insertvalue %11, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
  %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
  %22 = llvm.mlir.constant(0 : index) : i64
  %23 = llvm.insertvalue %22, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
  %24 = builtin.unrealized_conversion_cast %23 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
  llvm.store %cst, %18 : f32, !llvm.ptr
  %25 = llvm.load %18 : !llvm.ptr -> f32
  %26 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %27 = llvm.getelementptr %26[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  %28 = llvm.load %27 : !llvm.ptr -> i64
  %29 = builtin.unrealized_conversion_cast %28 : i64 to index
  %30 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %31 = llvm.getelementptr %30[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  %32 = llvm.load %31 : !llvm.ptr -> i64
  %33 = builtin.unrealized_conversion_cast %32 : i64 to index
  %34 = llvm.alloca %2 x f32 : (i64) -> !llvm.ptr
  llvm.store %25, %34 : f32, !llvm.ptr
  omp.parallel   {
    omp.wsloop   reduction(@__scf_reduction -> %34 : !llvm.ptr) for  (%arg4) : index = (%29) to (%33) step (%c1) {
      %36 = builtin.unrealized_conversion_cast %arg4 : index to i64
      %37 = llvm.intr.stacksave : !llvm.ptr
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %38 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %39 = llvm.getelementptr %38[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %40 = llvm.load %39 : !llvm.ptr -> f32
      omp.reduction %40, %34 : f32, !llvm.ptr
      llvm.intr.stackrestore %37 : !llvm.ptr
      llvm.br ^bb2
    ^bb2:  // pred: ^bb1
      omp.yield
    }
    omp.terminator
  }
  %35 = llvm.load %34 : !llvm.ptr -> f32
  llvm.store %35, %18 : f32, !llvm.ptr
  return %24 : memref<f32>
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %1 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %3 = builtin.unrealized_conversion_cast %c0 : index to i64
  %c1 = arith.constant 1 : index
  %4 = builtin.unrealized_conversion_cast %c1 : index to i64
  %cst = arith.constant 0.000000e+00 : f32
  %5 = llvm.mlir.constant(1 : index) : i64
  %6 = llvm.mlir.null : !llvm.ptr
  %7 = llvm.getelementptr %6[1] : (!llvm.ptr) -> !llvm.ptr, f32
  %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
  %9 = llvm.mlir.constant(64 : index) : i64
  %10 = llvm.add %8, %9  : i64
  %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
  %12 = llvm.ptrtoint %11 : !llvm.ptr to i64
  %13 = llvm.mlir.constant(1 : index) : i64
  %14 = llvm.sub %9, %13  : i64
  %15 = llvm.add %12, %14  : i64
  %16 = llvm.urem %15, %9  : i64
  %17 = llvm.sub %15, %16  : i64
  %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
  %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
  %20 = llvm.insertvalue %11, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
  %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
  %22 = llvm.mlir.constant(0 : index) : i64
  %23 = llvm.insertvalue %22, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
  %24 = builtin.unrealized_conversion_cast %23 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
  llvm.store %cst, %18 : f32, !llvm.ptr
  %25 = llvm.load %18 : !llvm.ptr -> f32
  %26 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %27 = llvm.getelementptr %26[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  %28 = llvm.load %27 : !llvm.ptr -> i64
  %29 = builtin.unrealized_conversion_cast %28 : i64 to index
  %30 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %31 = llvm.getelementptr %30[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  %32 = llvm.load %31 : !llvm.ptr -> i64
  %33 = builtin.unrealized_conversion_cast %32 : i64 to index
  %34 = llvm.alloca %2 x f32 : (i64) -> !llvm.ptr
  llvm.store %25, %34 : f32, !llvm.ptr
  omp.parallel   {
    omp.wsloop   reduction(@__scf_reduction -> %34 : !llvm.ptr) for  (%arg4) : index = (%29) to (%33) step (%c1) {
      %36 = builtin.unrealized_conversion_cast %arg4 : index to i64
      %37 = llvm.intr.stacksave : !llvm.ptr
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %38 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %39 = llvm.getelementptr %38[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %40 = llvm.load %39 : !llvm.ptr -> f32
      omp.reduction %40, %34 : f32, !llvm.ptr
      llvm.intr.stackrestore %37 : !llvm.ptr
      llvm.br ^bb2
    ^bb2:  // pred: ^bb1
      omp.yield
    }
    omp.terminator
  }
  %35 = llvm.load %34 : !llvm.ptr -> f32
  llvm.store %35, %18 : f32, !llvm.ptr
  return %24 : memref<f32>
}

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
  %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %1 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %3 = builtin.unrealized_conversion_cast %c0 : index to i64
  %c1 = arith.constant 1 : index
  %4 = builtin.unrealized_conversion_cast %c1 : index to i64
  %cst = arith.constant 0.000000e+00 : f32
  %5 = llvm.mlir.constant(1 : index) : i64
  %6 = llvm.mlir.null : !llvm.ptr
  %7 = llvm.getelementptr %6[1] : (!llvm.ptr) -> !llvm.ptr, f32
  %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
  %9 = llvm.mlir.constant(64 : index) : i64
  %10 = llvm.add %8, %9  : i64
  %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
  %12 = llvm.ptrtoint %11 : !llvm.ptr to i64
  %13 = llvm.mlir.constant(1 : index) : i64
  %14 = llvm.sub %9, %13  : i64
  %15 = llvm.add %12, %14  : i64
  %16 = llvm.urem %15, %9  : i64
  %17 = llvm.sub %15, %16  : i64
  %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
  %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
  %20 = llvm.insertvalue %11, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
  %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
  %22 = llvm.mlir.constant(0 : index) : i64
  %23 = llvm.insertvalue %22, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
  %24 = builtin.unrealized_conversion_cast %23 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
  llvm.store %cst, %18 : f32, !llvm.ptr
  %25 = llvm.load %18 : !llvm.ptr -> f32
  %26 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %27 = llvm.getelementptr %26[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  %28 = llvm.load %27 : !llvm.ptr -> i64
  %29 = builtin.unrealized_conversion_cast %28 : i64 to index
  %30 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %31 = llvm.getelementptr %30[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  %32 = llvm.load %31 : !llvm.ptr -> i64
  %33 = builtin.unrealized_conversion_cast %32 : i64 to index
  %34 = llvm.alloca %2 x f32 : (i64) -> !llvm.ptr
  llvm.store %25, %34 : f32, !llvm.ptr
  omp.parallel   {
    omp.wsloop   reduction(@__scf_reduction -> %34 : !llvm.ptr) for  (%arg4) : index = (%29) to (%33) step (%c1) {
      %36 = builtin.unrealized_conversion_cast %arg4 : index to i64
      %37 = llvm.intr.stacksave : !llvm.ptr
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %38 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %39 = llvm.getelementptr %38[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %40 = llvm.load %39 : !llvm.ptr -> f32
      omp.reduction %40, %34 : f32, !llvm.ptr
      llvm.intr.stackrestore %37 : !llvm.ptr
      llvm.br ^bb2
    ^bb2:  // pred: ^bb1
      omp.yield
    }
    omp.terminator
  }
  %35 = llvm.load %34 : !llvm.ptr -> f32
  llvm.store %35, %18 : f32, !llvm.ptr
  return %24 : memref<f32>
}

// -----// IR Dump After ConvertMathToLibm (convert-math-to-libm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %3 = builtin.unrealized_conversion_cast %c0 : index to i64
    %c1 = arith.constant 1 : index
    %4 = builtin.unrealized_conversion_cast %c1 : index to i64
    %cst = arith.constant 0.000000e+00 : f32
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = llvm.mlir.null : !llvm.ptr
    %7 = llvm.getelementptr %6[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.mlir.constant(64 : index) : i64
    %10 = llvm.add %8, %9  : i64
    %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
    %12 = llvm.ptrtoint %11 : !llvm.ptr to i64
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.sub %9, %13  : i64
    %15 = llvm.add %12, %14  : i64
    %16 = llvm.urem %15, %9  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %20 = llvm.insertvalue %11, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
    %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
    %22 = llvm.mlir.constant(0 : index) : i64
    %23 = llvm.insertvalue %22, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
    %24 = builtin.unrealized_conversion_cast %23 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %cst, %18 : f32, !llvm.ptr
    %25 = llvm.load %18 : !llvm.ptr -> f32
    %26 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %27 = llvm.getelementptr %26[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %28 = llvm.load %27 : !llvm.ptr -> i64
    %29 = builtin.unrealized_conversion_cast %28 : i64 to index
    %30 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %31 = llvm.getelementptr %30[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %32 = llvm.load %31 : !llvm.ptr -> i64
    %33 = builtin.unrealized_conversion_cast %32 : i64 to index
    %34 = llvm.alloca %2 x f32 : (i64) -> !llvm.ptr
    llvm.store %25, %34 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %34 : !llvm.ptr) for  (%arg4) : index = (%29) to (%33) step (%c1) {
        %36 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %37 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %38 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %40 = llvm.load %39 : !llvm.ptr -> f32
        omp.reduction %40, %34 : f32, !llvm.ptr
        llvm.intr.stackrestore %37 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %35 = llvm.load %34 : !llvm.ptr -> f32
    llvm.store %35, %18 : f32, !llvm.ptr
    return %24 : memref<f32>
  }
}


// -----// IR Dump After ConvertComplexToLibm (convert-complex-to-libm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %3 = builtin.unrealized_conversion_cast %c0 : index to i64
    %c1 = arith.constant 1 : index
    %4 = builtin.unrealized_conversion_cast %c1 : index to i64
    %cst = arith.constant 0.000000e+00 : f32
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = llvm.mlir.null : !llvm.ptr
    %7 = llvm.getelementptr %6[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.mlir.constant(64 : index) : i64
    %10 = llvm.add %8, %9  : i64
    %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
    %12 = llvm.ptrtoint %11 : !llvm.ptr to i64
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.sub %9, %13  : i64
    %15 = llvm.add %12, %14  : i64
    %16 = llvm.urem %15, %9  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %20 = llvm.insertvalue %11, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
    %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
    %22 = llvm.mlir.constant(0 : index) : i64
    %23 = llvm.insertvalue %22, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
    %24 = builtin.unrealized_conversion_cast %23 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %cst, %18 : f32, !llvm.ptr
    %25 = llvm.load %18 : !llvm.ptr -> f32
    %26 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %27 = llvm.getelementptr %26[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %28 = llvm.load %27 : !llvm.ptr -> i64
    %29 = builtin.unrealized_conversion_cast %28 : i64 to index
    %30 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %31 = llvm.getelementptr %30[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %32 = llvm.load %31 : !llvm.ptr -> i64
    %33 = builtin.unrealized_conversion_cast %32 : i64 to index
    %34 = llvm.alloca %2 x f32 : (i64) -> !llvm.ptr
    llvm.store %25, %34 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %34 : !llvm.ptr) for  (%arg4) : index = (%29) to (%33) step (%c1) {
        %36 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %37 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %38 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %40 = llvm.load %39 : !llvm.ptr -> f32
        omp.reduction %40, %34 : f32, !llvm.ptr
        llvm.intr.stackrestore %37 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %35 = llvm.load %34 : !llvm.ptr -> f32
    llvm.store %35, %18 : f32, !llvm.ptr
    return %24 : memref<f32>
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(0 : index) : i64
    %1 = llvm.mlir.constant(64 : index) : i64
    %2 = llvm.mlir.constant(1 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %5 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %6 = builtin.unrealized_conversion_cast %c0 : index to i64
    %7 = builtin.unrealized_conversion_cast %c1 : index to i64
    %8 = llvm.mlir.null : !llvm.ptr
    %9 = llvm.getelementptr %8[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.add %10, %1  : i64
    %12 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.sub %1, %2  : i64
    %15 = llvm.add %13, %14  : i64
    %16 = llvm.urem %15, %1  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %20 = llvm.insertvalue %12, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
    %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
    %22 = llvm.insertvalue %0, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
    %23 = builtin.unrealized_conversion_cast %22 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %cst, %18 : f32, !llvm.ptr
    %24 = llvm.load %18 : !llvm.ptr -> f32
    %25 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.getelementptr %25[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %27 = llvm.load %26 : !llvm.ptr -> i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %30 = llvm.getelementptr %29[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %31 = llvm.load %30 : !llvm.ptr -> i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
    llvm.store %24, %33 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %33 : !llvm.ptr) for  (%arg4) : index = (%28) to (%32) step (%c1) {
        %35 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %36 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %37 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %38 = llvm.getelementptr %37[%35] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %39 = llvm.load %38 : !llvm.ptr -> f32
        omp.reduction %39, %33 : f32, !llvm.ptr
        llvm.intr.stackrestore %36 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %34 = llvm.load %33 : !llvm.ptr -> f32
    llvm.store %34, %18 : f32, !llvm.ptr
    return %23 : memref<f32>
  }
}


// -----// IR Dump After ConvertComplexToLLVMPass (convert-complex-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(0 : index) : i64
    %1 = llvm.mlir.constant(64 : index) : i64
    %2 = llvm.mlir.constant(1 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %5 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %6 = builtin.unrealized_conversion_cast %c0 : index to i64
    %7 = builtin.unrealized_conversion_cast %c1 : index to i64
    %8 = llvm.mlir.null : !llvm.ptr
    %9 = llvm.getelementptr %8[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.add %10, %1  : i64
    %12 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.sub %1, %2  : i64
    %15 = llvm.add %13, %14  : i64
    %16 = llvm.urem %15, %1  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %20 = llvm.insertvalue %12, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
    %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
    %22 = llvm.insertvalue %0, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
    %23 = builtin.unrealized_conversion_cast %22 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %cst, %18 : f32, !llvm.ptr
    %24 = llvm.load %18 : !llvm.ptr -> f32
    %25 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.getelementptr %25[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %27 = llvm.load %26 : !llvm.ptr -> i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %30 = llvm.getelementptr %29[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %31 = llvm.load %30 : !llvm.ptr -> i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
    llvm.store %24, %33 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %33 : !llvm.ptr) for  (%arg4) : index = (%28) to (%32) step (%c1) {
        %35 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %36 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %37 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %38 = llvm.getelementptr %37[%35] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %39 = llvm.load %38 : !llvm.ptr -> f32
        omp.reduction %39, %33 : f32, !llvm.ptr
        llvm.intr.stackrestore %36 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %34 = llvm.load %33 : !llvm.ptr -> f32
    llvm.store %34, %18 : f32, !llvm.ptr
    return %23 : memref<f32>
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @Reduce1D.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> memref<f32> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(0 : index) : i64
    %1 = llvm.mlir.constant(64 : index) : i64
    %2 = llvm.mlir.constant(1 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %5 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %6 = builtin.unrealized_conversion_cast %c0 : index to i64
    %7 = builtin.unrealized_conversion_cast %c1 : index to i64
    %8 = llvm.mlir.null : !llvm.ptr
    %9 = llvm.getelementptr %8[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.add %10, %1  : i64
    %12 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.sub %1, %2  : i64
    %15 = llvm.add %13, %14  : i64
    %16 = llvm.urem %15, %1  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %20 = llvm.insertvalue %12, %19[0] : !llvm.struct<(ptr, ptr, i64)> 
    %21 = llvm.insertvalue %18, %20[1] : !llvm.struct<(ptr, ptr, i64)> 
    %22 = llvm.insertvalue %0, %21[2] : !llvm.struct<(ptr, ptr, i64)> 
    %23 = builtin.unrealized_conversion_cast %22 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %cst, %18 : f32, !llvm.ptr
    %24 = llvm.load %18 : !llvm.ptr -> f32
    %25 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.getelementptr %25[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %27 = llvm.load %26 : !llvm.ptr -> i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %30 = llvm.getelementptr %29[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %31 = llvm.load %30 : !llvm.ptr -> i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
    llvm.store %24, %33 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %33 : !llvm.ptr) for  (%arg4) : index = (%28) to (%32) step (%c1) {
        %35 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %36 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %37 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %38 = llvm.getelementptr %37[%35] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %39 = llvm.load %38 : !llvm.ptr -> f32
        omp.reduction %39, %33 : f32, !llvm.ptr
        llvm.intr.stackrestore %36 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %34 = llvm.load %33 : !llvm.ptr -> f32
    llvm.store %34, %18 : f32, !llvm.ptr
    return %23 : memref<f32>
  }
}


// -----// IR Dump After ConvertOpenMPToLLVMPass (convert-openmp-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = llvm.fadd %arg0, %arg1  : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  llvm.func @Reduce1D.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: !llvm.ptr, %arg6: !llvm.ptr, %arg7: i64, %arg8: i64, %arg9: i64, %arg10: !llvm.ptr, %arg11: !llvm.ptr, %arg12: i64, %arg13: i64, %arg14: i64, %arg15: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> !llvm.struct<(ptr, ptr, i64)> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = builtin.unrealized_conversion_cast %5 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %7 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.insertvalue %arg10, %7[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.insertvalue %arg11, %8[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.insertvalue %arg12, %9[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.insertvalue %arg13, %10[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.insertvalue %arg14, %11[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = builtin.unrealized_conversion_cast %12 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %14 = llvm.mlir.constant(0 : index) : i64
    %15 = llvm.mlir.constant(64 : index) : i64
    %16 = llvm.mlir.constant(1 : index) : i64
    %17 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %18 = llvm.mlir.constant(1 : index) : i64
    %19 = builtin.unrealized_conversion_cast %18 : i64 to index
    %20 = llvm.mlir.constant(0 : index) : i64
    %21 = builtin.unrealized_conversion_cast %20 : i64 to index
    %22 = llvm.mlir.constant(1 : i64) : i64
    %23 = builtin.unrealized_conversion_cast %6 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %24 = builtin.unrealized_conversion_cast %13 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %25 = builtin.unrealized_conversion_cast %21 : index to i64
    %26 = builtin.unrealized_conversion_cast %19 : index to i64
    %27 = llvm.mlir.null : !llvm.ptr
    %28 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %29 = llvm.ptrtoint %28 : !llvm.ptr to i64
    %30 = llvm.add %29, %15  : i64
    %31 = llvm.call @malloc(%30) : (i64) -> !llvm.ptr
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.sub %15, %16  : i64
    %34 = llvm.add %32, %33  : i64
    %35 = llvm.urem %34, %15  : i64
    %36 = llvm.sub %34, %35  : i64
    %37 = llvm.inttoptr %36 : i64 to !llvm.ptr
    %38 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %39 = llvm.insertvalue %31, %38[0] : !llvm.struct<(ptr, ptr, i64)> 
    %40 = llvm.insertvalue %37, %39[1] : !llvm.struct<(ptr, ptr, i64)> 
    %41 = llvm.insertvalue %14, %40[2] : !llvm.struct<(ptr, ptr, i64)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %17, %37 : f32, !llvm.ptr
    %43 = llvm.load %37 : !llvm.ptr -> f32
    %44 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %45 = llvm.getelementptr %44[%25] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %46 = llvm.load %45 : !llvm.ptr -> i64
    %47 = builtin.unrealized_conversion_cast %46 : i64 to index
    %48 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %49 = llvm.getelementptr %48[%26] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %50 = llvm.load %49 : !llvm.ptr -> i64
    %51 = builtin.unrealized_conversion_cast %50 : i64 to index
    %52 = llvm.alloca %22 x f32 : (i64) -> !llvm.ptr
    llvm.store %43, %52 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %52 : !llvm.ptr) for  (%arg16) : i64 = (%46) to (%50) step (%18) {
        %54 = builtin.unrealized_conversion_cast %arg16 : i64 to index
        %55 = builtin.unrealized_conversion_cast %54 : index to i64
        %56 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %57 = llvm.extractvalue %24[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %58 = llvm.getelementptr %57[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %59 = llvm.load %58 : !llvm.ptr -> f32
        omp.reduction %59, %52 : f32, !llvm.ptr
        llvm.intr.stackrestore %56 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %53 = llvm.load %52 : !llvm.ptr -> f32
    llvm.store %53, %37 : f32, !llvm.ptr
    llvm.return %41 : !llvm.struct<(ptr, ptr, i64)>
  }
  llvm.func @_mlir_ciface_Reduce1D.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.extractvalue %0[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %0[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.extractvalue %0[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.extractvalue %6[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.extractvalue %6[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.extractvalue %6[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.load %arg3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.extractvalue %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.extractvalue %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %12[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.extractvalue %12[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.extractvalue %12[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.call @Reduce1D.z.0.main(%1, %2, %3, %4, %5, %7, %8, %9, %10, %11, %13, %14, %15, %16, %17, %arg4) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> !llvm.struct<(ptr, ptr, i64)>
    llvm.store %18, %arg0 : !llvm.struct<(ptr, ptr, i64)>, !llvm.ptr
    llvm.return
  }
}


// -----// IR Dump After ConvertFuncToLLVMPass (convert-func-to-llvm) //----- //
module attributes {llvm.data_layout = ""} {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = llvm.fadd %arg0, %arg1  : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  llvm.func @Reduce1D.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: !llvm.ptr, %arg6: !llvm.ptr, %arg7: i64, %arg8: i64, %arg9: i64, %arg10: !llvm.ptr, %arg11: !llvm.ptr, %arg12: i64, %arg13: i64, %arg14: i64, %arg15: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> !llvm.struct<(ptr, ptr, i64)> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = builtin.unrealized_conversion_cast %5 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %7 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.insertvalue %arg10, %7[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.insertvalue %arg11, %8[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.insertvalue %arg12, %9[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.insertvalue %arg13, %10[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.insertvalue %arg14, %11[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = builtin.unrealized_conversion_cast %12 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %14 = llvm.mlir.constant(0 : index) : i64
    %15 = llvm.mlir.constant(64 : index) : i64
    %16 = llvm.mlir.constant(1 : index) : i64
    %17 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %18 = llvm.mlir.constant(1 : index) : i64
    %19 = builtin.unrealized_conversion_cast %18 : i64 to index
    %20 = llvm.mlir.constant(0 : index) : i64
    %21 = builtin.unrealized_conversion_cast %20 : i64 to index
    %22 = llvm.mlir.constant(1 : i64) : i64
    %23 = builtin.unrealized_conversion_cast %6 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %24 = builtin.unrealized_conversion_cast %13 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %25 = builtin.unrealized_conversion_cast %21 : index to i64
    %26 = builtin.unrealized_conversion_cast %19 : index to i64
    %27 = llvm.mlir.null : !llvm.ptr
    %28 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %29 = llvm.ptrtoint %28 : !llvm.ptr to i64
    %30 = llvm.add %29, %15  : i64
    %31 = llvm.call @malloc(%30) : (i64) -> !llvm.ptr
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.sub %15, %16  : i64
    %34 = llvm.add %32, %33  : i64
    %35 = llvm.urem %34, %15  : i64
    %36 = llvm.sub %34, %35  : i64
    %37 = llvm.inttoptr %36 : i64 to !llvm.ptr
    %38 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %39 = llvm.insertvalue %31, %38[0] : !llvm.struct<(ptr, ptr, i64)> 
    %40 = llvm.insertvalue %37, %39[1] : !llvm.struct<(ptr, ptr, i64)> 
    %41 = llvm.insertvalue %14, %40[2] : !llvm.struct<(ptr, ptr, i64)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64)> to memref<f32>
    llvm.store %17, %37 : f32, !llvm.ptr
    %43 = llvm.load %37 : !llvm.ptr -> f32
    %44 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %45 = llvm.getelementptr %44[%25] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %46 = llvm.load %45 : !llvm.ptr -> i64
    %47 = builtin.unrealized_conversion_cast %46 : i64 to index
    %48 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %49 = llvm.getelementptr %48[%26] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %50 = llvm.load %49 : !llvm.ptr -> i64
    %51 = builtin.unrealized_conversion_cast %50 : i64 to index
    %52 = llvm.alloca %22 x f32 : (i64) -> !llvm.ptr
    llvm.store %43, %52 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %52 : !llvm.ptr) for  (%arg16) : i64 = (%46) to (%50) step (%18) {
        %54 = builtin.unrealized_conversion_cast %arg16 : i64 to index
        %55 = builtin.unrealized_conversion_cast %54 : index to i64
        %56 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %57 = llvm.extractvalue %24[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %58 = llvm.getelementptr %57[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %59 = llvm.load %58 : !llvm.ptr -> f32
        omp.reduction %59, %52 : f32, !llvm.ptr
        llvm.intr.stackrestore %56 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %53 = llvm.load %52 : !llvm.ptr -> f32
    llvm.store %53, %37 : f32, !llvm.ptr
    llvm.return %41 : !llvm.struct<(ptr, ptr, i64)>
  }
  llvm.func @_mlir_ciface_Reduce1D.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.extractvalue %0[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %0[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.extractvalue %0[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.extractvalue %6[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.extractvalue %6[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.extractvalue %6[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.load %arg3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.extractvalue %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.extractvalue %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %12[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.extractvalue %12[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.extractvalue %12[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.call @Reduce1D.z.0.main(%1, %2, %3, %4, %5, %7, %8, %9, %10, %11, %13, %14, %15, %16, %17, %arg4) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> !llvm.struct<(ptr, ptr, i64)>
    llvm.store %18, %arg0 : !llvm.struct<(ptr, ptr, i64)>, !llvm.ptr
    llvm.return
  }
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {llvm.data_layout = ""} {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = llvm.fadd %arg0, %arg1  : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  llvm.func @Reduce1D.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: !llvm.ptr, %arg6: !llvm.ptr, %arg7: i64, %arg8: i64, %arg9: i64, %arg10: !llvm.ptr, %arg11: !llvm.ptr, %arg12: i64, %arg13: i64, %arg14: i64, %arg15: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> !llvm.struct<(ptr, ptr, i64)> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.insertvalue %arg10, %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.insertvalue %arg11, %7[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.insertvalue %arg12, %8[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.insertvalue %arg13, %9[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.insertvalue %arg14, %10[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.mlir.constant(0 : index) : i64
    %13 = llvm.mlir.constant(64 : index) : i64
    %14 = llvm.mlir.constant(1 : index) : i64
    %15 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %16 = llvm.mlir.constant(1 : index) : i64
    %17 = llvm.mlir.constant(0 : index) : i64
    %18 = llvm.mlir.constant(1 : i64) : i64
    %19 = llvm.mlir.null : !llvm.ptr
    %20 = llvm.getelementptr %19[1] : (!llvm.ptr) -> !llvm.ptr, f32
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %13  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %13, %14  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %13  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64)> 
    %33 = llvm.insertvalue %12, %32[2] : !llvm.struct<(ptr, ptr, i64)> 
    llvm.store %15, %29 : f32, !llvm.ptr
    %34 = llvm.load %29 : !llvm.ptr -> f32
    %35 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %36 = llvm.getelementptr %35[%17] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %37 = llvm.load %36 : !llvm.ptr -> i64
    %38 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %39 = llvm.getelementptr %38[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    %40 = llvm.load %39 : !llvm.ptr -> i64
    %41 = llvm.alloca %18 x f32 : (i64) -> !llvm.ptr
    llvm.store %34, %41 : f32, !llvm.ptr
    omp.parallel   {
      omp.wsloop   reduction(@__scf_reduction -> %41 : !llvm.ptr) for  (%arg16) : i64 = (%37) to (%40) step (%16) {
        %43 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %44 = llvm.extractvalue %11[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %45 = llvm.getelementptr %44[%arg16] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %46 = llvm.load %45 : !llvm.ptr -> f32
        omp.reduction %46, %41 : f32, !llvm.ptr
        llvm.intr.stackrestore %43 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %42 = llvm.load %41 : !llvm.ptr -> f32
    llvm.store %42, %29 : f32, !llvm.ptr
    llvm.return %33 : !llvm.struct<(ptr, ptr, i64)>
  }
  llvm.func @_mlir_ciface_Reduce1D.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.struct<(array<1 x i64>, array<3 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.extractvalue %0[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %0[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.extractvalue %0[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.extractvalue %6[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.extractvalue %6[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.extractvalue %6[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.load %arg3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.extractvalue %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.extractvalue %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %12[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.extractvalue %12[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.extractvalue %12[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.call @Reduce1D.z.0.main(%1, %2, %3, %4, %5, %7, %8, %9, %10, %11, %13, %14, %15, %16, %17, %arg4) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<1 x i64>, array<3 x i64>)>) -> !llvm.struct<(ptr, ptr, i64)>
    llvm.store %18, %arg0 : !llvm.struct<(ptr, ptr, i64)>, !llvm.ptr
    llvm.return
  }
}


