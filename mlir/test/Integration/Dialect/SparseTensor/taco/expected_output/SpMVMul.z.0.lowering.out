// -----// IR Dump After LinalgGeneralization (linalg-generalize-named-ops) //----- //
func.func @SpMVMul.z.0.main(%arg0: tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg1: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> attributes {llvm.emit_c_interface} {
  %0 = bufferization.alloc_tensor() : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0, %arg1 : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) outs(%0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.mulf %in, %in_0 : f32
    %3 = arith.addf %out, %2 : f32
    linalg.yield %3 : f32
  } -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  return %1 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
}

// -----// IR Dump After PreSparsificationRewrite (pre-sparsification-rewrite) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1)>
#map2 = affine_map<(d0, d1) -> (d0)>
module {
  func.func @SpMVMul.z.0.main(%arg0: tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg1: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> attributes {llvm.emit_c_interface} {
    %0 = bufferization.alloc_tensor() : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "reduction"]} ins(%arg0, %arg1 : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) outs(%0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.mulf %in, %in_0 : f32
      %3 = arith.addf %out, %2 : f32
      linalg.yield %3 : f32
    } -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    return %1 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  }
}


// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
func.func @SpMVMul.z.0.main(%arg0: tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg1: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> attributes {llvm.emit_c_interface} {
  %0 = bufferization.alloc_tensor() : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0, %arg1 : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) outs(%0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.mulf %in, %in_0 : f32
    %3 = arith.addf %out, %2 : f32
    linalg.yield %3 : f32
  } -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  return %1 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
}

// -----// IR Dump After SparsificationPass (sparsification) //----- //
module {
  func.func @SpMVMul.z.0.main(%arg0: tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg1: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> attributes {llvm.emit_c_interface} {
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %1 = sparse_tensor.positions %arg0 {level = 1 : index} : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>> to memref<?xindex>
    %2 = sparse_tensor.coordinates %arg0 {level = 1 : index} : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>> to memref<?xindex>
    %3 = sparse_tensor.values %arg0 : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>> to memref<?xf32>
    %4 = sparse_tensor.values %arg1 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> to memref<?xf32>
    %5 = sparse_tensor.values %0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> to memref<?xf32>
    scf.parallel (%arg2) = (%c0) to (%c128) step (%c1) {
      %7 = memref.load %5[%arg2] : memref<?xf32>
      %8 = memref.load %1[%arg2] : memref<?xindex>
      %9 = arith.addi %arg2, %c1 : index
      %10 = memref.load %1[%9] : memref<?xindex>
      %11 = scf.parallel (%arg3) = (%8) to (%10) step (%c1) init (%7) -> f32 {
        %12 = memref.load %2[%arg3] : memref<?xindex>
        %13 = memref.load %3[%arg3] : memref<?xf32>
        %14 = memref.load %4[%12] : memref<?xf32>
        %15 = arith.mulf %13, %14 : f32
        scf.reduce(%15)  : f32 {
        ^bb0(%arg4: f32, %arg5: f32):
          %16 = arith.addf %arg4, %arg5 : f32
          scf.reduce.return %16 : f32
        }
        scf.yield
      } {"Emitted from" = "linalg.generic"}
      memref.store %11, %5[%arg2] : memref<?xf32>
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    %6 = sparse_tensor.load %0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    return %6 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  }
}


// -----// IR Dump After PostSparsificationRewrite (post-sparsification-rewrite) //----- //
module {
  func.func @SpMVMul.z.0.main(%arg0: tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg1: tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> attributes {llvm.emit_c_interface} {
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %0 = bufferization.alloc_tensor() {bufferization.escape = [true]} : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %1 = sparse_tensor.positions %arg0 {level = 1 : index} : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>> to memref<?xindex>
    %2 = sparse_tensor.coordinates %arg0 {level = 1 : index} : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>> to memref<?xindex>
    %3 = sparse_tensor.values %arg0 : tensor<128x128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>> to memref<?xf32>
    %4 = sparse_tensor.values %arg1 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> to memref<?xf32>
    %5 = sparse_tensor.values %0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> to memref<?xf32>
    scf.parallel (%arg2) = (%c0) to (%c128) step (%c1) {
      %7 = memref.load %5[%arg2] : memref<?xf32>
      %8 = memref.load %1[%arg2] : memref<?xindex>
      %9 = arith.addi %arg2, %c1 : index
      %10 = memref.load %1[%9] : memref<?xindex>
      %11 = scf.parallel (%arg3) = (%8) to (%10) step (%c1) init (%7) -> f32 {
        %12 = memref.load %2[%arg3] : memref<?xindex>
        %13 = memref.load %3[%arg3] : memref<?xf32>
        %14 = memref.load %4[%12] : memref<?xf32>
        %15 = arith.mulf %13, %14 : f32
        scf.reduce(%15)  : f32 {
        ^bb0(%arg4: f32, %arg5: f32):
          %16 = arith.addf %arg4, %arg5 : f32
          scf.reduce.return %16 : f32
        }
        scf.yield
      } {"Emitted from" = "linalg.generic"}
      memref.store %11, %5[%arg2] : memref<?xf32>
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    %6 = sparse_tensor.load %0 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    return %6 : tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  }
}


// -----// IR Dump After SparseTensorCodegen (sparse-tensor-codegen) //----- //
module {
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg4: memref<?xf32>, %arg5: !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> (memref<?xf32>, !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) attributes {llvm.emit_c_interface} {
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c128_0 = arith.constant 128 : index
    %alloc = memref.alloc(%c128_0) : memref<?xf32>
    %0 = sparse_tensor.storage_specifier.init : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %c0_1 = arith.constant 0 : index
    %1 = sparse_tensor.storage_specifier.set %0  lvl_sz at 0 with %c128_0 : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %c1_2 = arith.constant 1 : index
    %c128_3 = arith.constant 128 : index
    %2 = arith.muli %c1_2, %c128_3 : index
    %cst = arith.constant 0.000000e+00 : f32
    %3 = sparse_tensor.storage_specifier.get %1  val_mem_sz : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %outBuffer, %newSize = sparse_tensor.push_back %3, %alloc, %cst, %2 : index, memref<?xf32>, f32, index
    %4 = sparse_tensor.storage_specifier.set %1  val_mem_sz with %newSize : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %5 = builtin.unrealized_conversion_cast %outBuffer, %4 : memref<?xf32>, !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> to tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
      %7 = memref.load %outBuffer[%arg6] : memref<?xf32>
      %8 = memref.load %arg0[%arg6] : memref<?xindex>
      %9 = arith.addi %arg6, %c1 : index
      %10 = memref.load %arg0[%9] : memref<?xindex>
      %11 = scf.parallel (%arg7) = (%8) to (%10) step (%c1) init (%7) -> f32 {
        %12 = memref.load %arg1[%arg7] : memref<?xindex>
        %13 = memref.load %arg2[%arg7] : memref<?xf32>
        %14 = memref.load %arg4[%12] : memref<?xf32>
        %15 = arith.mulf %13, %14 : f32
        scf.reduce(%15)  : f32 {
        ^bb0(%arg8: f32, %arg9: f32):
          %16 = arith.addf %arg8, %arg9 : f32
          scf.reduce.return %16 : f32
        }
        scf.yield
      } {"Emitted from" = "linalg.generic"}
      memref.store %11, %outBuffer[%arg6] : memref<?xf32>
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    %6 = builtin.unrealized_conversion_cast %outBuffer, %4 : memref<?xf32>, !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>> to tensor<128xf32, #sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    return %outBuffer, %4 : memref<?xf32>, !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  }
}


// -----// IR Dump After SparseBufferRewrite (sparse-buffer-rewrite) //----- //
module {
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense", "compressed" ] }>>, %arg4: memref<?xf32>, %arg5: !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) -> (memref<?xf32>, !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>) attributes {llvm.emit_c_interface} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %alloc = memref.alloc(%c128) : memref<?xf32>
    %0 = sparse_tensor.storage_specifier.init : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %1 = sparse_tensor.storage_specifier.set %0  lvl_sz at 0 with %c128 : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %2 = sparse_tensor.storage_specifier.get %1  val_mem_sz : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    %3 = arith.addi %2, %c128 : index
    %4 = arith.cmpi ugt, %3, %c128 : index
    %5 = scf.if %4 -> (memref<?xf32>) {
      %7 = scf.while (%arg6 = %c128) : (index) -> index {
        %9 = arith.muli %arg6, %c2 : index
        %10 = arith.cmpi ugt, %3, %9 : index
        scf.condition(%10) %9 : index
      } do {
      ^bb0(%arg6: index):
        scf.yield %arg6 : index
      }
      %8 = memref.realloc %alloc(%7) : memref<?xf32> to memref<?xf32>
      scf.yield %8 : memref<?xf32>
    } else {
      scf.yield %alloc : memref<?xf32>
    }
    %subview = memref.subview %5[%2] [%c128] [%c1] : memref<?xf32> to memref<?xf32, strided<[?], offset: ?>>
    linalg.fill ins(%cst : f32) outs(%subview : memref<?xf32, strided<[?], offset: ?>>)
    %6 = sparse_tensor.storage_specifier.set %1  val_mem_sz with %3 : !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
    scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
      %7 = memref.load %5[%arg6] : memref<?xf32>
      %8 = memref.load %arg0[%arg6] : memref<?xindex>
      %9 = arith.addi %arg6, %c1 : index
      %10 = memref.load %arg0[%9] : memref<?xindex>
      %11 = scf.parallel (%arg7) = (%8) to (%10) step (%c1) init (%7) -> f32 {
        %12 = memref.load %arg1[%arg7] : memref<?xindex>
        %13 = memref.load %arg2[%arg7] : memref<?xf32>
        %14 = memref.load %arg4[%12] : memref<?xf32>
        %15 = arith.mulf %13, %14 : f32
        scf.reduce(%15)  : f32 {
        ^bb0(%arg8: f32, %arg9: f32):
          %16 = arith.addf %arg8, %arg9 : f32
          scf.reduce.return %16 : f32
        }
        scf.yield
      } {"Emitted from" = "linalg.generic"}
      memref.store %11, %5[%arg6] : memref<?xf32>
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    return %5, %6 : memref<?xf32>, !sparse_tensor.storage_specifier<#sparse_tensor.encoding<{ lvlTypes = [ "dense" ] }>>
  }
}


// -----// IR Dump After StorageSpecifierToLLVM (sparse-storage-specifier-to-llvm) //----- //
module {
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %alloc = memref.alloc(%c128) : memref<?xf32>
    %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %c0_i64 = arith.constant 0 : i64
    %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %2 = arith.index_cast %c128 : index to i64
    %3 = llvm.insertvalue %2, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %5 = arith.index_cast %4 : i64 to index
    %6 = arith.addi %5, %c128 : index
    %7 = arith.cmpi ugt, %6, %c128 : index
    %8 = scf.if %7 -> (memref<?xf32>) {
      %11 = scf.while (%arg6 = %c128) : (index) -> index {
        %13 = arith.muli %arg6, %c2 : index
        %14 = arith.cmpi ugt, %6, %13 : index
        scf.condition(%14) %13 : index
      } do {
      ^bb0(%arg6: index):
        scf.yield %arg6 : index
      }
      %12 = memref.realloc %alloc(%11) : memref<?xf32> to memref<?xf32>
      scf.yield %12 : memref<?xf32>
    } else {
      scf.yield %alloc : memref<?xf32>
    }
    %subview = memref.subview %8[%5] [%c128] [%c1] : memref<?xf32> to memref<?xf32, strided<[?], offset: ?>>
    linalg.fill ins(%cst : f32) outs(%subview : memref<?xf32, strided<[?], offset: ?>>)
    %9 = arith.index_cast %6 : index to i64
    %10 = llvm.insertvalue %9, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
      %11 = memref.load %8[%arg6] : memref<?xf32>
      %12 = memref.load %arg0[%arg6] : memref<?xindex>
      %13 = arith.addi %arg6, %c1 : index
      %14 = memref.load %arg0[%13] : memref<?xindex>
      %15 = scf.parallel (%arg7) = (%12) to (%14) step (%c1) init (%11) -> f32 {
        %16 = memref.load %arg1[%arg7] : memref<?xindex>
        %17 = memref.load %arg2[%arg7] : memref<?xf32>
        %18 = memref.load %arg4[%16] : memref<?xf32>
        %19 = arith.mulf %17, %18 : f32
        scf.reduce(%19)  : f32 {
        ^bb0(%arg8: f32, %arg9: f32):
          %20 = arith.addf %arg8, %arg9 : f32
          scf.reduce.return %20 : f32
        }
        scf.yield
      } {"Emitted from" = "linalg.generic"}
      memref.store %15, %8[%arg6] : memref<?xf32>
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    return %8, %10 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After mlir::sparse_tensor::SparsificationAndBufferizationPass () //----- //
module {
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %alloc = memref.alloc(%c128) : memref<?xf32>
    %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %c0_i64 = arith.constant 0 : i64
    %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %2 = arith.index_cast %c128 : index to i64
    %3 = llvm.insertvalue %2, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %5 = arith.index_cast %4 : i64 to index
    %6 = arith.addi %5, %c128 : index
    %7 = arith.cmpi ugt, %6, %c128 : index
    %8 = scf.if %7 -> (memref<?xf32>) {
      %11 = scf.while (%arg6 = %c128) : (index) -> index {
        %13 = arith.muli %arg6, %c2 : index
        %14 = arith.cmpi ugt, %6, %13 : index
        scf.condition(%14) %13 : index
      } do {
      ^bb0(%arg6: index):
        scf.yield %arg6 : index
      }
      %12 = memref.realloc %alloc(%11) : memref<?xf32> to memref<?xf32>
      scf.yield %12 : memref<?xf32>
    } else {
      scf.yield %alloc : memref<?xf32>
    }
    %subview = memref.subview %8[%5] [%c128] [%c1] : memref<?xf32> to memref<?xf32, strided<[?], offset: ?>>
    linalg.fill ins(%cst : f32) outs(%subview : memref<?xf32, strided<[?], offset: ?>>)
    %9 = arith.index_cast %6 : index to i64
    %10 = llvm.insertvalue %9, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
      %11 = memref.load %8[%arg6] : memref<?xf32>
      %12 = memref.load %arg0[%arg6] : memref<?xindex>
      %13 = arith.addi %arg6, %c1 : index
      %14 = memref.load %arg0[%13] : memref<?xindex>
      %15 = scf.parallel (%arg7) = (%12) to (%14) step (%c1) init (%11) -> f32 {
        %16 = memref.load %arg1[%arg7] : memref<?xindex>
        %17 = memref.load %arg2[%arg7] : memref<?xf32>
        %18 = memref.load %arg4[%16] : memref<?xf32>
        %19 = arith.mulf %17, %18 : f32
        scf.reduce(%19)  : f32 {
        ^bb0(%arg8: f32, %arg9: f32):
          %20 = arith.addf %arg8, %arg9 : f32
          scf.reduce.return %20 : f32
        }
        scf.yield
      } {"Emitted from" = "linalg.generic"}
      memref.store %15, %8[%arg6] : memref<?xf32>
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    return %8, %10 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %c128_i64 = arith.constant 128 : i64
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c0_i64 = arith.constant 0 : i64
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() : memref<128xf32>
  %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
  %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %2 = llvm.insertvalue %c128_i64, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  linalg.fill ins(%cst : f32) outs(%alloc : memref<128xf32>)
  %3 = llvm.insertvalue %c128_i64, %2[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
    %4 = memref.load %alloc[%arg6] : memref<128xf32>
    %5 = memref.load %arg0[%arg6] : memref<?xindex>
    %6 = arith.addi %arg6, %c1 : index
    %7 = memref.load %arg0[%6] : memref<?xindex>
    %8 = scf.parallel (%arg7) = (%5) to (%7) step (%c1) init (%4) -> f32 {
      %9 = memref.load %arg1[%arg7] : memref<?xindex>
      %10 = memref.load %arg2[%arg7] : memref<?xf32>
      %11 = memref.load %arg4[%9] : memref<?xf32>
      %12 = arith.mulf %10, %11 : f32
      scf.reduce(%12)  : f32 {
      ^bb0(%arg8: f32, %arg9: f32):
        %13 = arith.addf %arg8, %arg9 : f32
        scf.reduce.return %13 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %8, %alloc[%arg6] : memref<128xf32>
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  return %cast, %3 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After FinalizingBufferize (finalizing-bufferize) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %c128_i64 = arith.constant 128 : i64
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c0_i64 = arith.constant 0 : i64
  %c1 = arith.constant 1 : index
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() : memref<128xf32>
  %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
  %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %2 = llvm.insertvalue %c128_i64, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  linalg.fill ins(%cst : f32) outs(%alloc : memref<128xf32>)
  %3 = llvm.insertvalue %c128_i64, %2[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
    %4 = memref.load %alloc[%arg6] : memref<128xf32>
    %5 = memref.load %arg0[%arg6] : memref<?xindex>
    %6 = arith.addi %arg6, %c1 : index
    %7 = memref.load %arg0[%6] : memref<?xindex>
    %8 = scf.parallel (%arg7) = (%5) to (%7) step (%c1) init (%4) -> f32 {
      %9 = memref.load %arg1[%arg7] : memref<?xindex>
      %10 = memref.load %arg2[%arg7] : memref<?xf32>
      %11 = memref.load %arg4[%9] : memref<?xf32>
      %12 = arith.mulf %10, %11 : f32
      scf.reduce(%12)  : f32 {
      ^bb0(%arg8: f32, %arg9: f32):
        %13 = arith.addf %arg8, %arg9 : f32
        scf.reduce.return %13 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %8, %alloc[%arg6] : memref<128xf32>
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  return %cast, %3 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() : memref<128xf32>
  %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
  %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %2 = llvm.insertvalue %c128_i64, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.for %arg6 = %c0 to %c128 step %c1 {
    memref.store %cst, %alloc[%arg6] : memref<128xf32>
  }
  %3 = llvm.insertvalue %c128_i64, %2[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
    %4 = memref.load %alloc[%arg6] : memref<128xf32>
    %5 = memref.load %arg0[%arg6] : memref<?xindex>
    %6 = arith.addi %arg6, %c1 : index
    %7 = memref.load %arg0[%6] : memref<?xindex>
    %8 = scf.parallel (%arg7) = (%5) to (%7) step (%c1) init (%4) -> f32 {
      %9 = memref.load %arg1[%arg7] : memref<?xindex>
      %10 = memref.load %arg2[%arg7] : memref<?xf32>
      %11 = memref.load %arg4[%9] : memref<?xf32>
      %12 = arith.mulf %10, %11 : f32
      scf.reduce(%12)  : f32 {
      ^bb0(%arg8: f32, %arg9: f32):
        %13 = arith.addf %arg8, %arg9 : f32
        scf.reduce.return %13 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %8, %alloc[%arg6] : memref<128xf32>
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  return %cast, %3 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() : memref<128xf32>
  %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
  %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %2 = llvm.insertvalue %c128_i64, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.for %arg6 = %c0 to %c128 step %c1 {
    memref.store %cst, %alloc[%arg6] : memref<128xf32>
  }
  %3 = llvm.insertvalue %c128_i64, %2[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.parallel (%arg6) = (%c0) to (%c128) step (%c1) {
    %4 = memref.load %alloc[%arg6] : memref<128xf32>
    %5 = memref.load %arg0[%arg6] : memref<?xindex>
    %6 = arith.addi %arg6, %c1 : index
    %7 = memref.load %arg0[%6] : memref<?xindex>
    %8 = scf.parallel (%arg7) = (%5) to (%7) step (%c1) init (%4) -> f32 {
      %9 = memref.load %arg1[%arg7] : memref<?xindex>
      %10 = memref.load %arg2[%arg7] : memref<?xf32>
      %11 = memref.load %arg4[%9] : memref<?xf32>
      %12 = arith.mulf %10, %11 : f32
      scf.reduce(%12)  : f32 {
      ^bb0(%arg8: f32, %arg9: f32):
        %13 = arith.addf %arg8, %arg9 : f32
        scf.reduce.return %13 : f32
      }
      scf.yield
    } {"Emitted from" = "linalg.generic"}
    memref.store %8, %alloc[%arg6] : memref<128xf32>
    scf.yield
  } {"Emitted from" = "linalg.generic"}
  return %cast, %3 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After ConvertSCFToOpenMPPass (convert-scf-to-openmp) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() : memref<128xf32>
    %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
    %0 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %c0_i64, %0[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %c128_i64, %1[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    scf.for %arg6 = %c0 to %c128 step %c1 {
      memref.store %cst, %alloc[%arg6] : memref<128xf32>
    }
    %3 = llvm.insertvalue %c128_i64, %2[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel   {
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        memref.alloca_scope  {
          %5 = memref.load %alloc[%arg6] : memref<128xf32>
          %6 = memref.load %arg0[%arg6] : memref<?xindex>
          %7 = arith.addi %arg6, %c1 : index
          %8 = memref.load %arg0[%7] : memref<?xindex>
          %9 = llvm.mlir.constant(1 : i64) : i64
          %10 = llvm.alloca %9 x f32 : (i64) -> !llvm.ptr
          llvm.store %5, %10 : f32, !llvm.ptr
          omp.parallel   {
            omp.wsloop   reduction(@__scf_reduction -> %10 : !llvm.ptr) for  (%arg7) : index = (%6) to (%8) step (%c1) {
              memref.alloca_scope  {
                %12 = memref.load %arg1[%arg7] : memref<?xindex>
                %13 = memref.load %arg2[%arg7] : memref<?xf32>
                %14 = memref.load %arg4[%12] : memref<?xf32>
                %15 = arith.mulf %13, %14 : f32
                omp.reduction %15, %10 : f32, !llvm.ptr
              }
              omp.yield
            }
            omp.terminator
          }
          %11 = llvm.load %10 : !llvm.ptr -> f32
          memref.store %11, %alloc[%arg6] : memref<128xf32>
        }
        omp.yield
      }
      omp.terminator
    }
    return %cast, %3 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %0 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() : memref<128xf32>
  %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
  %1 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %2 = llvm.insertvalue %c0_i64, %1[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %3 = llvm.insertvalue %c128_i64, %2[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  scf.for %arg6 = %c0 to %c128 step %c1 {
    memref.store %cst, %alloc[%arg6] : memref<128xf32>
  }
  %4 = llvm.insertvalue %c128_i64, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  omp.parallel   {
    %5 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
    omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
      %6 = memref.load %alloc[%arg6] : memref<128xf32>
      %7 = memref.load %arg0[%arg6] : memref<?xindex>
      %8 = arith.addi %arg6, %c1 : index
      %9 = memref.load %arg0[%8] : memref<?xindex>
      llvm.store %6, %5 : f32, !llvm.ptr
      omp.parallel   {
        omp.wsloop   reduction(@__scf_reduction -> %5 : !llvm.ptr) for  (%arg7) : index = (%7) to (%9) step (%c1) {
          memref.alloca_scope  {
            %11 = memref.load %arg1[%arg7] : memref<?xindex>
            %12 = memref.load %arg2[%arg7] : memref<?xf32>
            %13 = memref.load %arg4[%11] : memref<?xf32>
            %14 = arith.mulf %12, %13 : f32
            omp.reduction %14, %5 : f32, !llvm.ptr
          }
          omp.yield
        }
        omp.terminator
      }
      %10 = llvm.load %5 : !llvm.ptr -> f32
      memref.store %10, %alloc[%arg6] : memref<128xf32>
      omp.yield
    }
    omp.terminator
  }
  return %cast, %4 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %0 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %alloc = memref.alloc() : memref<128xf32>
  %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
  %1 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %2 = llvm.insertvalue %c0_i64, %1[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %3 = llvm.insertvalue %c128_i64, %2[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  cf.br ^bb1(%c0 : index)
^bb1(%4: index):  // 2 preds: ^bb0, ^bb2
  %5 = arith.cmpi slt, %4, %c128 : index
  cf.cond_br %5, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  memref.store %cst, %alloc[%4] : memref<128xf32>
  %6 = arith.addi %4, %c1 : index
  cf.br ^bb1(%6 : index)
^bb3:  // pred: ^bb1
  %7 = llvm.insertvalue %c128_i64, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  omp.parallel   {
    %8 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
    omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
      %9 = memref.load %alloc[%arg6] : memref<128xf32>
      %10 = memref.load %arg0[%arg6] : memref<?xindex>
      %11 = arith.addi %arg6, %c1 : index
      %12 = memref.load %arg0[%11] : memref<?xindex>
      llvm.store %9, %8 : f32, !llvm.ptr
      omp.parallel   {
        omp.wsloop   reduction(@__scf_reduction -> %8 : !llvm.ptr) for  (%arg7) : index = (%10) to (%12) step (%c1) {
          memref.alloca_scope  {
            %14 = memref.load %arg1[%arg7] : memref<?xindex>
            %15 = memref.load %arg2[%arg7] : memref<?xf32>
            %16 = memref.load %arg4[%14] : memref<?xf32>
            %17 = arith.mulf %15, %16 : f32
            omp.reduction %17, %8 : f32, !llvm.ptr
          }
          omp.yield
        }
        omp.terminator
      }
      %13 = llvm.load %8 : !llvm.ptr -> f32
      memref.store %13, %alloc[%arg6] : memref<128xf32>
      omp.yield
    }
    omp.terminator
  }
  return %cast, %7 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() : memref<128xf32>
    %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
    %1 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %2 = llvm.insertvalue %c0_i64, %1[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %c128_i64, %2[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%4: index):  // 2 preds: ^bb0, ^bb2
    %5 = arith.cmpi slt, %4, %c128 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    memref.store %cst, %alloc[%4] : memref<128xf32>
    %6 = arith.addi %4, %c1 : index
    cf.br ^bb1(%6 : index)
  ^bb3:  // pred: ^bb1
    %7 = llvm.insertvalue %c128_i64, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %8 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %9 = memref.load %alloc[%arg6] : memref<128xf32>
        %10 = memref.load %arg0[%arg6] : memref<?xindex>
        %11 = arith.addi %arg6, %c1 : index
        %12 = memref.load %arg0[%11] : memref<?xindex>
        llvm.store %9, %8 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %8 : !llvm.ptr) for  (%arg7) : index = (%10) to (%12) step (%c1) {
            memref.alloca_scope  {
              %14 = memref.load %arg1[%arg7] : memref<?xindex>
              %15 = memref.load %arg2[%arg7] : memref<?xf32>
              %16 = memref.load %arg4[%14] : memref<?xf32>
              %17 = arith.mulf %15, %16 : f32
              omp.reduction %17, %8 : f32, !llvm.ptr
            }
            omp.yield
          }
          omp.terminator
        }
        %13 = llvm.load %8 : !llvm.ptr -> f32
        memref.store %13, %alloc[%arg6] : memref<128xf32>
        omp.yield
      }
      omp.terminator
    }
    return %cast, %7 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() : memref<128xf32>
    %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
    %1 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %2 = llvm.insertvalue %c0_i64, %1[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %c128_i64, %2[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%4: index):  // 2 preds: ^bb0, ^bb2
    %5 = arith.cmpi slt, %4, %c128 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    memref.store %cst, %alloc[%4] : memref<128xf32>
    %6 = arith.addi %4, %c1 : index
    cf.br ^bb1(%6 : index)
  ^bb3:  // pred: ^bb1
    %7 = llvm.insertvalue %c128_i64, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %8 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %9 = memref.load %alloc[%arg6] : memref<128xf32>
        %10 = memref.load %arg0[%arg6] : memref<?xindex>
        %11 = arith.addi %arg6, %c1 : index
        %12 = memref.load %arg0[%11] : memref<?xindex>
        llvm.store %9, %8 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %8 : !llvm.ptr) for  (%arg7) : index = (%10) to (%12) step (%c1) {
            memref.alloca_scope  {
              %14 = memref.load %arg1[%arg7] : memref<?xindex>
              %15 = memref.load %arg2[%arg7] : memref<?xf32>
              %16 = memref.load %arg4[%14] : memref<?xf32>
              %17 = arith.mulf %15, %16 : f32
              omp.reduction %17, %8 : f32, !llvm.ptr
            }
            omp.yield
          }
          omp.terminator
        }
        %13 = llvm.load %8 : !llvm.ptr -> f32
        memref.store %13, %alloc[%arg6] : memref<128xf32>
        omp.yield
      }
      omp.terminator
    }
    return %cast, %7 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %alloc = memref.alloc() : memref<128xf32>
    %cast = memref.cast %alloc : memref<128xf32> to memref<?xf32>
    %1 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %2 = llvm.insertvalue %c0_i64, %1[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %c128_i64, %2[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%4: index):  // 2 preds: ^bb0, ^bb2
    %5 = arith.cmpi slt, %4, %c128 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    memref.store %cst, %alloc[%4] : memref<128xf32>
    %6 = arith.addi %4, %c1 : index
    cf.br ^bb1(%6 : index)
  ^bb3:  // pred: ^bb1
    %7 = llvm.insertvalue %c128_i64, %3[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %8 = llvm.alloca %0 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %9 = memref.load %alloc[%arg6] : memref<128xf32>
        %10 = memref.load %arg0[%arg6] : memref<?xindex>
        %11 = arith.addi %arg6, %c1 : index
        %12 = memref.load %arg0[%11] : memref<?xindex>
        llvm.store %9, %8 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %8 : !llvm.ptr) for  (%arg7) : index = (%10) to (%12) step (%c1) {
            memref.alloca_scope  {
              %14 = memref.load %arg1[%arg7] : memref<?xindex>
              %15 = memref.load %arg2[%arg7] : memref<?xf32>
              %16 = memref.load %arg4[%14] : memref<?xf32>
              %17 = arith.mulf %15, %16 : f32
              omp.reduction %17, %8 : f32, !llvm.ptr
            }
            omp.yield
          }
          omp.terminator
        }
        %13 = llvm.load %8 : !llvm.ptr -> f32
        memref.store %13, %alloc[%arg6] : memref<128xf32>
        omp.yield
      }
      omp.terminator
    }
    return %cast, %7 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %2 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %3 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %4 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %5 = llvm.mlir.constant(128 : index) : i64
    %6 = llvm.mlir.constant(1 : index) : i64
    %7 = llvm.mlir.null : !llvm.ptr
    %8 = llvm.getelementptr %7[%5] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %9 = llvm.ptrtoint %8 : !llvm.ptr to i64
    %10 = llvm.call @malloc(%9) : (i64) -> !llvm.ptr
    %11 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %12 = llvm.insertvalue %10, %11[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = llvm.insertvalue %10, %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.mlir.constant(0 : index) : i64
    %15 = llvm.insertvalue %14, %13[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %5, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %6, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
    %23 = builtin.unrealized_conversion_cast %22 : index to i64
    %24 = arith.cmpi slt, %22, %c128 : index
    cf.cond_br %24, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %25 = llvm.extractvalue %17[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.getelementptr %25[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %cst, %26 : f32, !llvm.ptr
    %27 = arith.addi %22, %c1 : index
    cf.br ^bb1(%27 : index)
  ^bb3:  // pred: ^bb1
    %28 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %29 = llvm.alloca %4 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %30 = builtin.unrealized_conversion_cast %arg6 : index to i64
        %31 = llvm.extractvalue %17[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %32 = llvm.getelementptr %31[%30] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %33 = llvm.load %32 : !llvm.ptr -> f32
        %34 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %35 = llvm.getelementptr %34[%30] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %36 = llvm.load %35 : !llvm.ptr -> i64
        %37 = builtin.unrealized_conversion_cast %36 : i64 to index
        %38 = arith.addi %arg6, %c1 : index
        %39 = builtin.unrealized_conversion_cast %38 : index to i64
        %40 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %41 = llvm.getelementptr %40[%39] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %42 = llvm.load %41 : !llvm.ptr -> i64
        %43 = builtin.unrealized_conversion_cast %42 : i64 to index
        llvm.store %33, %29 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %29 : !llvm.ptr) for  (%arg7) : index = (%37) to (%43) step (%c1) {
            %47 = builtin.unrealized_conversion_cast %arg7 : index to i64
            %48 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %49 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %50 = llvm.getelementptr %49[%47] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %51 = llvm.load %50 : !llvm.ptr -> i64
            %52 = llvm.extractvalue %2[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %53 = llvm.getelementptr %52[%47] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %54 = llvm.load %53 : !llvm.ptr -> f32
            %55 = llvm.extractvalue %3[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %56 = llvm.getelementptr %55[%51] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %57 = llvm.load %56 : !llvm.ptr -> f32
            %58 = arith.mulf %54, %57 : f32
            omp.reduction %58, %29 : f32, !llvm.ptr
            llvm.intr.stackrestore %48 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %44 = llvm.load %29 : !llvm.ptr -> f32
        %45 = llvm.extractvalue %17[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %46 = llvm.getelementptr %45[%30] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %44, %46 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    return %18, %28 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %1 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %2 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %3 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %4 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %5 = llvm.mlir.constant(128 : index) : i64
  %6 = llvm.mlir.constant(1 : index) : i64
  %7 = llvm.mlir.null : !llvm.ptr
  %8 = llvm.getelementptr %7[128] : (!llvm.ptr) -> !llvm.ptr, f32
  %9 = llvm.ptrtoint %8 : !llvm.ptr to i64
  %10 = llvm.call @malloc(%9) : (i64) -> !llvm.ptr
  %11 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %12 = llvm.insertvalue %10, %11[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %13 = llvm.insertvalue %10, %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %14 = llvm.mlir.constant(0 : index) : i64
  %15 = llvm.insertvalue %14, %13[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %16 = llvm.insertvalue %5, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %17 = llvm.insertvalue %6, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
  %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  cf.br ^bb1(%c0 : index)
^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
  %23 = builtin.unrealized_conversion_cast %22 : index to i64
  %24 = arith.cmpi slt, %22, %c128 : index
  cf.cond_br %24, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %25 = llvm.getelementptr %10[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
  llvm.store %cst, %25 : f32, !llvm.ptr
  %26 = arith.addi %22, %c1 : index
  cf.br ^bb1(%26 : index)
^bb3:  // pred: ^bb1
  %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  omp.parallel   {
    %28 = llvm.alloca %4 x f32 : (i64) -> !llvm.ptr
    omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
      %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
      %30 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %31 = llvm.load %30 : !llvm.ptr -> f32
      %32 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
      %34 = llvm.load %33 : !llvm.ptr -> i64
      %35 = builtin.unrealized_conversion_cast %34 : i64 to index
      %36 = arith.addi %arg6, %c1 : index
      %37 = builtin.unrealized_conversion_cast %36 : index to i64
      %38 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
      %40 = llvm.load %39 : !llvm.ptr -> i64
      %41 = builtin.unrealized_conversion_cast %40 : i64 to index
      llvm.store %31, %28 : f32, !llvm.ptr
      omp.parallel   {
        omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
          %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
          %45 = llvm.intr.stacksave : !llvm.ptr
          llvm.br ^bb1
        ^bb1:  // pred: ^bb0
          %46 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
          %48 = llvm.load %47 : !llvm.ptr -> i64
          %49 = llvm.extractvalue %2[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
          %51 = llvm.load %50 : !llvm.ptr -> f32
          %52 = llvm.extractvalue %3[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
          %54 = llvm.load %53 : !llvm.ptr -> f32
          %55 = arith.mulf %51, %54 : f32
          omp.reduction %55, %28 : f32, !llvm.ptr
          llvm.intr.stackrestore %45 : !llvm.ptr
          llvm.br ^bb2
        ^bb2:  // pred: ^bb1
          omp.yield
        }
        omp.terminator
      }
      %42 = llvm.load %28 : !llvm.ptr -> f32
      %43 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %42, %43 : f32, !llvm.ptr
      omp.yield
    }
    omp.terminator
  }
  return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %1 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %2 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %3 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %4 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %5 = llvm.mlir.constant(128 : index) : i64
  %6 = llvm.mlir.constant(1 : index) : i64
  %7 = llvm.mlir.null : !llvm.ptr
  %8 = llvm.getelementptr %7[128] : (!llvm.ptr) -> !llvm.ptr, f32
  %9 = llvm.ptrtoint %8 : !llvm.ptr to i64
  %10 = llvm.call @malloc(%9) : (i64) -> !llvm.ptr
  %11 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %12 = llvm.insertvalue %10, %11[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %13 = llvm.insertvalue %10, %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %14 = llvm.mlir.constant(0 : index) : i64
  %15 = llvm.insertvalue %14, %13[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %16 = llvm.insertvalue %5, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %17 = llvm.insertvalue %6, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
  %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  cf.br ^bb1(%c0 : index)
^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
  %23 = builtin.unrealized_conversion_cast %22 : index to i64
  %24 = arith.cmpi slt, %22, %c128 : index
  cf.cond_br %24, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %25 = llvm.getelementptr %10[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
  llvm.store %cst, %25 : f32, !llvm.ptr
  %26 = arith.addi %22, %c1 : index
  cf.br ^bb1(%26 : index)
^bb3:  // pred: ^bb1
  %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  omp.parallel   {
    %28 = llvm.alloca %4 x f32 : (i64) -> !llvm.ptr
    omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
      %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
      %30 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %31 = llvm.load %30 : !llvm.ptr -> f32
      %32 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
      %34 = llvm.load %33 : !llvm.ptr -> i64
      %35 = builtin.unrealized_conversion_cast %34 : i64 to index
      %36 = arith.addi %arg6, %c1 : index
      %37 = builtin.unrealized_conversion_cast %36 : index to i64
      %38 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
      %40 = llvm.load %39 : !llvm.ptr -> i64
      %41 = builtin.unrealized_conversion_cast %40 : i64 to index
      llvm.store %31, %28 : f32, !llvm.ptr
      omp.parallel   {
        omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
          %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
          %45 = llvm.intr.stacksave : !llvm.ptr
          llvm.br ^bb1
        ^bb1:  // pred: ^bb0
          %46 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
          %48 = llvm.load %47 : !llvm.ptr -> i64
          %49 = llvm.extractvalue %2[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
          %51 = llvm.load %50 : !llvm.ptr -> f32
          %52 = llvm.extractvalue %3[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
          %54 = llvm.load %53 : !llvm.ptr -> f32
          %55 = arith.mulf %51, %54 : f32
          omp.reduction %55, %28 : f32, !llvm.ptr
          llvm.intr.stackrestore %45 : !llvm.ptr
          llvm.br ^bb2
        ^bb2:  // pred: ^bb1
          omp.yield
        }
        omp.terminator
      }
      %42 = llvm.load %28 : !llvm.ptr -> f32
      %43 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %42, %43 : f32, !llvm.ptr
      omp.yield
    }
    omp.terminator
  }
  return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
  %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %1 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %2 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %3 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %4 = llvm.mlir.constant(1 : i64) : i64
  %c0 = arith.constant 0 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c128_i64 = arith.constant 128 : i64
  %c0_i64 = arith.constant 0 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %5 = llvm.mlir.constant(128 : index) : i64
  %6 = llvm.mlir.constant(1 : index) : i64
  %7 = llvm.mlir.null : !llvm.ptr
  %8 = llvm.getelementptr %7[128] : (!llvm.ptr) -> !llvm.ptr, f32
  %9 = llvm.ptrtoint %8 : !llvm.ptr to i64
  %10 = llvm.call @malloc(%9) : (i64) -> !llvm.ptr
  %11 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %12 = llvm.insertvalue %10, %11[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %13 = llvm.insertvalue %10, %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %14 = llvm.mlir.constant(0 : index) : i64
  %15 = llvm.insertvalue %14, %13[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %16 = llvm.insertvalue %5, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %17 = llvm.insertvalue %6, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
  %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  cf.br ^bb1(%c0 : index)
^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
  %23 = builtin.unrealized_conversion_cast %22 : index to i64
  %24 = arith.cmpi slt, %22, %c128 : index
  cf.cond_br %24, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %25 = llvm.getelementptr %10[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
  llvm.store %cst, %25 : f32, !llvm.ptr
  %26 = arith.addi %22, %c1 : index
  cf.br ^bb1(%26 : index)
^bb3:  // pred: ^bb1
  %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
  omp.parallel   {
    %28 = llvm.alloca %4 x f32 : (i64) -> !llvm.ptr
    omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
      %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
      %30 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %31 = llvm.load %30 : !llvm.ptr -> f32
      %32 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
      %34 = llvm.load %33 : !llvm.ptr -> i64
      %35 = builtin.unrealized_conversion_cast %34 : i64 to index
      %36 = arith.addi %arg6, %c1 : index
      %37 = builtin.unrealized_conversion_cast %36 : index to i64
      %38 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
      %40 = llvm.load %39 : !llvm.ptr -> i64
      %41 = builtin.unrealized_conversion_cast %40 : i64 to index
      llvm.store %31, %28 : f32, !llvm.ptr
      omp.parallel   {
        omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
          %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
          %45 = llvm.intr.stacksave : !llvm.ptr
          llvm.br ^bb1
        ^bb1:  // pred: ^bb0
          %46 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
          %48 = llvm.load %47 : !llvm.ptr -> i64
          %49 = llvm.extractvalue %2[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
          %51 = llvm.load %50 : !llvm.ptr -> f32
          %52 = llvm.extractvalue %3[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
          %54 = llvm.load %53 : !llvm.ptr -> f32
          %55 = arith.mulf %51, %54 : f32
          omp.reduction %55, %28 : f32, !llvm.ptr
          llvm.intr.stackrestore %45 : !llvm.ptr
          llvm.br ^bb2
        ^bb2:  // pred: ^bb1
          omp.yield
        }
        omp.terminator
      }
      %42 = llvm.load %28 : !llvm.ptr -> f32
      %43 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %42, %43 : f32, !llvm.ptr
      omp.yield
    }
    omp.terminator
  }
  return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
}

// -----// IR Dump After ConvertMathToLibm (convert-math-to-libm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %2 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %3 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %4 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %5 = llvm.mlir.constant(128 : index) : i64
    %6 = llvm.mlir.constant(1 : index) : i64
    %7 = llvm.mlir.null : !llvm.ptr
    %8 = llvm.getelementptr %7[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %9 = llvm.ptrtoint %8 : !llvm.ptr to i64
    %10 = llvm.call @malloc(%9) : (i64) -> !llvm.ptr
    %11 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %12 = llvm.insertvalue %10, %11[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = llvm.insertvalue %10, %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.mlir.constant(0 : index) : i64
    %15 = llvm.insertvalue %14, %13[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %5, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %6, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
    %23 = builtin.unrealized_conversion_cast %22 : index to i64
    %24 = arith.cmpi slt, %22, %c128 : index
    cf.cond_br %24, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %25 = llvm.getelementptr %10[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %cst, %25 : f32, !llvm.ptr
    %26 = arith.addi %22, %c1 : index
    cf.br ^bb1(%26 : index)
  ^bb3:  // pred: ^bb1
    %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %28 = llvm.alloca %4 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
        %30 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %31 = llvm.load %30 : !llvm.ptr -> f32
        %32 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %34 = llvm.load %33 : !llvm.ptr -> i64
        %35 = builtin.unrealized_conversion_cast %34 : i64 to index
        %36 = arith.addi %arg6, %c1 : index
        %37 = builtin.unrealized_conversion_cast %36 : index to i64
        %38 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %40 = llvm.load %39 : !llvm.ptr -> i64
        %41 = builtin.unrealized_conversion_cast %40 : i64 to index
        llvm.store %31, %28 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
            %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
            %45 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %46 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %48 = llvm.load %47 : !llvm.ptr -> i64
            %49 = llvm.extractvalue %2[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %51 = llvm.load %50 : !llvm.ptr -> f32
            %52 = llvm.extractvalue %3[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %54 = llvm.load %53 : !llvm.ptr -> f32
            %55 = arith.mulf %51, %54 : f32
            omp.reduction %55, %28 : f32, !llvm.ptr
            llvm.intr.stackrestore %45 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %42 = llvm.load %28 : !llvm.ptr -> f32
        %43 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %42, %43 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertComplexToLibm (convert-complex-to-libm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %2 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %3 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %4 = llvm.mlir.constant(1 : i64) : i64
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c128_i64 = arith.constant 128 : i64
    %c0_i64 = arith.constant 0 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %5 = llvm.mlir.constant(128 : index) : i64
    %6 = llvm.mlir.constant(1 : index) : i64
    %7 = llvm.mlir.null : !llvm.ptr
    %8 = llvm.getelementptr %7[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %9 = llvm.ptrtoint %8 : !llvm.ptr to i64
    %10 = llvm.call @malloc(%9) : (i64) -> !llvm.ptr
    %11 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %12 = llvm.insertvalue %10, %11[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = llvm.insertvalue %10, %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.mlir.constant(0 : index) : i64
    %15 = llvm.insertvalue %14, %13[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %5, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %6, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
    %23 = builtin.unrealized_conversion_cast %22 : index to i64
    %24 = arith.cmpi slt, %22, %c128 : index
    cf.cond_br %24, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %25 = llvm.getelementptr %10[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %cst, %25 : f32, !llvm.ptr
    %26 = arith.addi %22, %c1 : index
    cf.br ^bb1(%26 : index)
  ^bb3:  // pred: ^bb1
    %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %28 = llvm.alloca %4 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
        %30 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %31 = llvm.load %30 : !llvm.ptr -> f32
        %32 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %34 = llvm.load %33 : !llvm.ptr -> i64
        %35 = builtin.unrealized_conversion_cast %34 : i64 to index
        %36 = arith.addi %arg6, %c1 : index
        %37 = builtin.unrealized_conversion_cast %36 : index to i64
        %38 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %40 = llvm.load %39 : !llvm.ptr -> i64
        %41 = builtin.unrealized_conversion_cast %40 : i64 to index
        llvm.store %31, %28 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
            %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
            %45 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %46 = llvm.extractvalue %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %48 = llvm.load %47 : !llvm.ptr -> i64
            %49 = llvm.extractvalue %2[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %51 = llvm.load %50 : !llvm.ptr -> f32
            %52 = llvm.extractvalue %3[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %54 = llvm.load %53 : !llvm.ptr -> f32
            %55 = arith.mulf %51, %54 : f32
            omp.reduction %55, %28 : f32, !llvm.ptr
            llvm.intr.stackrestore %45 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %42 = llvm.load %28 : !llvm.ptr -> f32
        %43 = llvm.getelementptr %10[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %42, %43 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(0 : index) : i64
    %1 = llvm.mlir.constant(1 : index) : i64
    %2 = llvm.mlir.constant(128 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i64 = arith.constant 0 : i64
    %c128_i64 = arith.constant 128 : i64
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %5 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %6 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.mlir.null : !llvm.ptr
    %9 = llvm.getelementptr %8[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
    %12 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.insertvalue %11, %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.insertvalue %11, %13[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.insertvalue %0, %14[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %2, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %1, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
    %23 = builtin.unrealized_conversion_cast %22 : index to i64
    %24 = arith.cmpi slt, %22, %c128 : index
    cf.cond_br %24, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %25 = llvm.getelementptr %11[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %cst, %25 : f32, !llvm.ptr
    %26 = arith.addi %22, %c1 : index
    cf.br ^bb1(%26 : index)
  ^bb3:  // pred: ^bb1
    %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %28 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
        %30 = llvm.getelementptr %11[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %31 = llvm.load %30 : !llvm.ptr -> f32
        %32 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %34 = llvm.load %33 : !llvm.ptr -> i64
        %35 = builtin.unrealized_conversion_cast %34 : i64 to index
        %36 = arith.addi %arg6, %c1 : index
        %37 = builtin.unrealized_conversion_cast %36 : index to i64
        %38 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %40 = llvm.load %39 : !llvm.ptr -> i64
        %41 = builtin.unrealized_conversion_cast %40 : i64 to index
        llvm.store %31, %28 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
            %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
            %45 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %46 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %48 = llvm.load %47 : !llvm.ptr -> i64
            %49 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %51 = llvm.load %50 : !llvm.ptr -> f32
            %52 = llvm.extractvalue %7[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %54 = llvm.load %53 : !llvm.ptr -> f32
            %55 = arith.mulf %51, %54 : f32
            omp.reduction %55, %28 : f32, !llvm.ptr
            llvm.intr.stackrestore %45 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %42 = llvm.load %28 : !llvm.ptr -> f32
        %43 = llvm.getelementptr %11[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %42, %43 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertComplexToLLVMPass (convert-complex-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(0 : index) : i64
    %1 = llvm.mlir.constant(1 : index) : i64
    %2 = llvm.mlir.constant(128 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i64 = arith.constant 0 : i64
    %c128_i64 = arith.constant 128 : i64
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %5 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %6 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.mlir.null : !llvm.ptr
    %9 = llvm.getelementptr %8[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
    %12 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.insertvalue %11, %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.insertvalue %11, %13[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.insertvalue %0, %14[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %2, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %1, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
    %23 = builtin.unrealized_conversion_cast %22 : index to i64
    %24 = arith.cmpi slt, %22, %c128 : index
    cf.cond_br %24, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %25 = llvm.getelementptr %11[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %cst, %25 : f32, !llvm.ptr
    %26 = arith.addi %22, %c1 : index
    cf.br ^bb1(%26 : index)
  ^bb3:  // pred: ^bb1
    %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %28 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
        %30 = llvm.getelementptr %11[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %31 = llvm.load %30 : !llvm.ptr -> f32
        %32 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %34 = llvm.load %33 : !llvm.ptr -> i64
        %35 = builtin.unrealized_conversion_cast %34 : i64 to index
        %36 = arith.addi %arg6, %c1 : index
        %37 = builtin.unrealized_conversion_cast %36 : index to i64
        %38 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %40 = llvm.load %39 : !llvm.ptr -> i64
        %41 = builtin.unrealized_conversion_cast %40 : i64 to index
        llvm.store %31, %28 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
            %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
            %45 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %46 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %48 = llvm.load %47 : !llvm.ptr -> i64
            %49 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %51 = llvm.load %50 : !llvm.ptr -> f32
            %52 = llvm.extractvalue %7[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %54 = llvm.load %53 : !llvm.ptr -> f32
            %55 = arith.mulf %51, %54 : f32
            omp.reduction %55, %28 : f32, !llvm.ptr
            llvm.intr.stackrestore %45 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %42 = llvm.load %28 : !llvm.ptr -> f32
        %43 = llvm.getelementptr %11[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %42, %43 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = arith.addf %arg0, %arg1 : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  func.func @SpMVMul.z.0.main(%arg0: memref<?xindex>, %arg1: memref<?xindex>, %arg2: memref<?xf32>, %arg3: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg4: memref<?xf32>, %arg5: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> (memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(0 : index) : i64
    %1 = llvm.mlir.constant(1 : index) : i64
    %2 = llvm.mlir.constant(128 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c0_i64 = arith.constant 0 : i64
    %c128_i64 = arith.constant 128 : i64
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %3 = llvm.mlir.constant(1 : i64) : i64
    %4 = builtin.unrealized_conversion_cast %arg0 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %5 = builtin.unrealized_conversion_cast %arg1 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %6 = builtin.unrealized_conversion_cast %arg2 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = builtin.unrealized_conversion_cast %arg4 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.mlir.null : !llvm.ptr
    %9 = llvm.getelementptr %8[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.call @malloc(%10) : (i64) -> !llvm.ptr
    %12 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.insertvalue %11, %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.insertvalue %11, %13[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.insertvalue %0, %14[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %2, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %1, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = builtin.unrealized_conversion_cast %17 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %19 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %20 = llvm.insertvalue %c0_i64, %19[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %c128_i64, %20[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%22: index):  // 2 preds: ^bb0, ^bb2
    %23 = builtin.unrealized_conversion_cast %22 : index to i64
    %24 = arith.cmpi slt, %22, %c128 : index
    cf.cond_br %24, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %25 = llvm.getelementptr %11[%23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %cst, %25 : f32, !llvm.ptr
    %26 = arith.addi %22, %c1 : index
    cf.br ^bb1(%26 : index)
  ^bb3:  // pred: ^bb1
    %27 = llvm.insertvalue %c128_i64, %21[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %28 = llvm.alloca %3 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg6) : index = (%c0) to (%c128) step (%c1) {
        %29 = builtin.unrealized_conversion_cast %arg6 : index to i64
        %30 = llvm.getelementptr %11[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %31 = llvm.load %30 : !llvm.ptr -> f32
        %32 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %33 = llvm.getelementptr %32[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %34 = llvm.load %33 : !llvm.ptr -> i64
        %35 = builtin.unrealized_conversion_cast %34 : i64 to index
        %36 = arith.addi %arg6, %c1 : index
        %37 = builtin.unrealized_conversion_cast %36 : index to i64
        %38 = llvm.extractvalue %4[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %39 = llvm.getelementptr %38[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %40 = llvm.load %39 : !llvm.ptr -> i64
        %41 = builtin.unrealized_conversion_cast %40 : i64 to index
        llvm.store %31, %28 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %28 : !llvm.ptr) for  (%arg7) : index = (%35) to (%41) step (%c1) {
            %44 = builtin.unrealized_conversion_cast %arg7 : index to i64
            %45 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %46 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %47 = llvm.getelementptr %46[%44] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %48 = llvm.load %47 : !llvm.ptr -> i64
            %49 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %50 = llvm.getelementptr %49[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %51 = llvm.load %50 : !llvm.ptr -> f32
            %52 = llvm.extractvalue %7[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %53 = llvm.getelementptr %52[%48] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %54 = llvm.load %53 : !llvm.ptr -> f32
            %55 = arith.mulf %51, %54 : f32
            omp.reduction %55, %28 : f32, !llvm.ptr
            llvm.intr.stackrestore %45 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %42 = llvm.load %28 : !llvm.ptr -> f32
        %43 = llvm.getelementptr %11[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %42, %43 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    return %18, %27 : memref<?xf32>, !llvm.struct<(array<1 x i64>, array<1 x i64>)>
  }
}


// -----// IR Dump After ConvertOpenMPToLLVMPass (convert-openmp-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = llvm.fadd %arg0, %arg1  : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  llvm.func @SpMVMul.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: !llvm.ptr, %arg6: !llvm.ptr, %arg7: i64, %arg8: i64, %arg9: i64, %arg10: !llvm.ptr, %arg11: !llvm.ptr, %arg12: i64, %arg13: i64, %arg14: i64, %arg15: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg16: !llvm.ptr, %arg17: !llvm.ptr, %arg18: i64, %arg19: i64, %arg20: i64, %arg21: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = builtin.unrealized_conversion_cast %5 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %7 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.insertvalue %arg5, %7[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.insertvalue %arg6, %8[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.insertvalue %arg7, %9[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.insertvalue %arg8, %10[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.insertvalue %arg9, %11[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = builtin.unrealized_conversion_cast %12 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %14 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %15 = llvm.insertvalue %arg10, %14[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %arg11, %15[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %arg12, %16[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.insertvalue %arg13, %17[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %19 = llvm.insertvalue %arg14, %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %20 = builtin.unrealized_conversion_cast %19 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %21 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %22 = llvm.insertvalue %arg16, %21[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %23 = llvm.insertvalue %arg17, %22[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %24 = llvm.insertvalue %arg18, %23[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %25 = llvm.insertvalue %arg19, %24[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.insertvalue %arg20, %25[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %27 = builtin.unrealized_conversion_cast %26 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %28 = llvm.mlir.constant(0 : index) : i64
    %29 = llvm.mlir.constant(1 : index) : i64
    %30 = llvm.mlir.constant(128 : index) : i64
    %31 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %32 = llvm.mlir.constant(0 : i64) : i64
    %33 = llvm.mlir.constant(128 : i64) : i64
    %34 = llvm.mlir.constant(1 : index) : i64
    %35 = llvm.mlir.constant(128 : index) : i64
    %36 = llvm.mlir.constant(0 : index) : i64
    %37 = llvm.mlir.constant(1 : i64) : i64
    %38 = builtin.unrealized_conversion_cast %6 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %39 = builtin.unrealized_conversion_cast %13 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = builtin.unrealized_conversion_cast %20 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %41 = builtin.unrealized_conversion_cast %27 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %42 = llvm.mlir.null : !llvm.ptr
    %43 = llvm.getelementptr %42[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %44 = llvm.ptrtoint %43 : !llvm.ptr to i64
    %45 = llvm.call @malloc(%44) : (i64) -> !llvm.ptr
    %46 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %47 = llvm.insertvalue %45, %46[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %48 = llvm.insertvalue %45, %47[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %49 = llvm.insertvalue %28, %48[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %50 = llvm.insertvalue %30, %49[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %51 = llvm.insertvalue %29, %50[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %52 = builtin.unrealized_conversion_cast %51 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %53 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %54 = llvm.insertvalue %32, %53[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %55 = llvm.insertvalue %33, %54[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    llvm.br ^bb1(%36 : i64)
  ^bb1(%56: i64):  // 2 preds: ^bb0, ^bb2
    %57 = builtin.unrealized_conversion_cast %56 : i64 to index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.icmp "slt" %56, %35 : i64
    llvm.cond_br %59, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %60 = llvm.getelementptr %45[%58] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %31, %60 : f32, !llvm.ptr
    %61 = llvm.add %56, %34  : i64
    llvm.br ^bb1(%61 : i64)
  ^bb3:  // pred: ^bb1
    %62 = llvm.insertvalue %33, %55[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %66 = llvm.alloca %37 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg22) : i64 = (%36) to (%35) step (%34) {
        %67 = builtin.unrealized_conversion_cast %arg22 : i64 to index
        %68 = builtin.unrealized_conversion_cast %67 : index to i64
        %69 = llvm.getelementptr %45[%68] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %70 = llvm.load %69 : !llvm.ptr -> f32
        %71 = llvm.extractvalue %38[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %72 = llvm.getelementptr %71[%68] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %73 = llvm.load %72 : !llvm.ptr -> i64
        %74 = builtin.unrealized_conversion_cast %73 : i64 to index
        %75 = llvm.add %arg22, %34  : i64
        %76 = builtin.unrealized_conversion_cast %75 : i64 to index
        %77 = builtin.unrealized_conversion_cast %76 : index to i64
        %78 = llvm.extractvalue %38[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %79 = llvm.getelementptr %78[%77] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %80 = llvm.load %79 : !llvm.ptr -> i64
        %81 = builtin.unrealized_conversion_cast %80 : i64 to index
        llvm.store %70, %66 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %66 : !llvm.ptr) for  (%arg23) : i64 = (%73) to (%80) step (%34) {
            %84 = builtin.unrealized_conversion_cast %arg23 : i64 to index
            %85 = builtin.unrealized_conversion_cast %84 : index to i64
            %86 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %87 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %88 = llvm.getelementptr %87[%85] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %89 = llvm.load %88 : !llvm.ptr -> i64
            %90 = llvm.extractvalue %40[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %91 = llvm.getelementptr %90[%85] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %92 = llvm.load %91 : !llvm.ptr -> f32
            %93 = llvm.extractvalue %41[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %94 = llvm.getelementptr %93[%89] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %95 = llvm.load %94 : !llvm.ptr -> f32
            %96 = llvm.fmul %92, %95  : f32
            omp.reduction %96, %66 : f32, !llvm.ptr
            llvm.intr.stackrestore %86 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %82 = llvm.load %66 : !llvm.ptr -> f32
        %83 = llvm.getelementptr %45[%68] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %82, %83 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    %63 = llvm.mlir.undef : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
    %64 = llvm.insertvalue %51, %63[0] : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> 
    %65 = llvm.insertvalue %62, %64[1] : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> 
    llvm.return %65 : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
  }
  llvm.func @_mlir_ciface_SpMVMul.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg5: !llvm.ptr, %arg6: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.extractvalue %0[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %0[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.extractvalue %0[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.extractvalue %6[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.extractvalue %6[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.extractvalue %6[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.load %arg3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.extractvalue %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.extractvalue %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %12[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.extractvalue %12[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.extractvalue %12[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.load %arg5 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %20 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %22 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %23 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %24 = llvm.call @SpMVMul.z.0.main(%1, %2, %3, %4, %5, %7, %8, %9, %10, %11, %13, %14, %15, %16, %17, %arg4, %19, %20, %21, %22, %23, %arg6) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<2 x i64>, array<3 x i64>)>, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
    llvm.store %24, %arg0 : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>, !llvm.ptr
    llvm.return
  }
}


// -----// IR Dump After ConvertFuncToLLVMPass (convert-func-to-llvm) //----- //
module attributes {llvm.data_layout = ""} {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = llvm.fadd %arg0, %arg1  : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  llvm.func @SpMVMul.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: !llvm.ptr, %arg6: !llvm.ptr, %arg7: i64, %arg8: i64, %arg9: i64, %arg10: !llvm.ptr, %arg11: !llvm.ptr, %arg12: i64, %arg13: i64, %arg14: i64, %arg15: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg16: !llvm.ptr, %arg17: !llvm.ptr, %arg18: i64, %arg19: i64, %arg20: i64, %arg21: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = builtin.unrealized_conversion_cast %5 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %7 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = llvm.insertvalue %arg5, %7[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.insertvalue %arg6, %8[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.insertvalue %arg7, %9[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.insertvalue %arg8, %10[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.insertvalue %arg9, %11[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = builtin.unrealized_conversion_cast %12 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %14 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %15 = llvm.insertvalue %arg10, %14[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %arg11, %15[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %arg12, %16[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.insertvalue %arg13, %17[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %19 = llvm.insertvalue %arg14, %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %20 = builtin.unrealized_conversion_cast %19 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %21 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %22 = llvm.insertvalue %arg16, %21[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %23 = llvm.insertvalue %arg17, %22[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %24 = llvm.insertvalue %arg18, %23[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %25 = llvm.insertvalue %arg19, %24[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.insertvalue %arg20, %25[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %27 = builtin.unrealized_conversion_cast %26 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %28 = llvm.mlir.constant(0 : index) : i64
    %29 = llvm.mlir.constant(1 : index) : i64
    %30 = llvm.mlir.constant(128 : index) : i64
    %31 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %32 = llvm.mlir.constant(0 : i64) : i64
    %33 = llvm.mlir.constant(128 : i64) : i64
    %34 = llvm.mlir.constant(1 : index) : i64
    %35 = llvm.mlir.constant(128 : index) : i64
    %36 = llvm.mlir.constant(0 : index) : i64
    %37 = llvm.mlir.constant(1 : i64) : i64
    %38 = builtin.unrealized_conversion_cast %6 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %39 = builtin.unrealized_conversion_cast %13 : memref<?xindex> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = builtin.unrealized_conversion_cast %20 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %41 = builtin.unrealized_conversion_cast %27 : memref<?xf32> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %42 = llvm.mlir.null : !llvm.ptr
    %43 = llvm.getelementptr %42[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %44 = llvm.ptrtoint %43 : !llvm.ptr to i64
    %45 = llvm.call @malloc(%44) : (i64) -> !llvm.ptr
    %46 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %47 = llvm.insertvalue %45, %46[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %48 = llvm.insertvalue %45, %47[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %49 = llvm.insertvalue %28, %48[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %50 = llvm.insertvalue %30, %49[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %51 = llvm.insertvalue %29, %50[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %52 = builtin.unrealized_conversion_cast %51 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf32>
    %53 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %54 = llvm.insertvalue %32, %53[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %55 = llvm.insertvalue %33, %54[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    llvm.br ^bb1(%36 : i64)
  ^bb1(%56: i64):  // 2 preds: ^bb0, ^bb2
    %57 = builtin.unrealized_conversion_cast %56 : i64 to index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.icmp "slt" %56, %35 : i64
    llvm.cond_br %59, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %60 = llvm.getelementptr %45[%58] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %31, %60 : f32, !llvm.ptr
    %61 = llvm.add %56, %34  : i64
    llvm.br ^bb1(%61 : i64)
  ^bb3:  // pred: ^bb1
    %62 = llvm.insertvalue %33, %55[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %66 = llvm.alloca %37 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg22) : i64 = (%36) to (%35) step (%34) {
        %67 = builtin.unrealized_conversion_cast %arg22 : i64 to index
        %68 = builtin.unrealized_conversion_cast %67 : index to i64
        %69 = llvm.getelementptr %45[%68] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %70 = llvm.load %69 : !llvm.ptr -> f32
        %71 = llvm.extractvalue %38[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %72 = llvm.getelementptr %71[%68] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %73 = llvm.load %72 : !llvm.ptr -> i64
        %74 = builtin.unrealized_conversion_cast %73 : i64 to index
        %75 = llvm.add %arg22, %34  : i64
        %76 = builtin.unrealized_conversion_cast %75 : i64 to index
        %77 = builtin.unrealized_conversion_cast %76 : index to i64
        %78 = llvm.extractvalue %38[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %79 = llvm.getelementptr %78[%77] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %80 = llvm.load %79 : !llvm.ptr -> i64
        %81 = builtin.unrealized_conversion_cast %80 : i64 to index
        llvm.store %70, %66 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %66 : !llvm.ptr) for  (%arg23) : i64 = (%73) to (%80) step (%34) {
            %84 = builtin.unrealized_conversion_cast %arg23 : i64 to index
            %85 = builtin.unrealized_conversion_cast %84 : index to i64
            %86 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %87 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %88 = llvm.getelementptr %87[%85] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %89 = llvm.load %88 : !llvm.ptr -> i64
            %90 = llvm.extractvalue %40[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %91 = llvm.getelementptr %90[%85] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %92 = llvm.load %91 : !llvm.ptr -> f32
            %93 = llvm.extractvalue %41[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %94 = llvm.getelementptr %93[%89] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %95 = llvm.load %94 : !llvm.ptr -> f32
            %96 = llvm.fmul %92, %95  : f32
            omp.reduction %96, %66 : f32, !llvm.ptr
            llvm.intr.stackrestore %86 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %82 = llvm.load %66 : !llvm.ptr -> f32
        %83 = llvm.getelementptr %45[%68] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %82, %83 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    %63 = llvm.mlir.undef : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
    %64 = llvm.insertvalue %51, %63[0] : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> 
    %65 = llvm.insertvalue %62, %64[1] : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> 
    llvm.return %65 : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
  }
  llvm.func @_mlir_ciface_SpMVMul.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg5: !llvm.ptr, %arg6: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.extractvalue %0[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %0[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.extractvalue %0[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.extractvalue %6[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.extractvalue %6[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.extractvalue %6[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.load %arg3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.extractvalue %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.extractvalue %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %12[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.extractvalue %12[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.extractvalue %12[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.load %arg5 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %20 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %22 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %23 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %24 = llvm.call @SpMVMul.z.0.main(%1, %2, %3, %4, %5, %7, %8, %9, %10, %11, %13, %14, %15, %16, %17, %arg4, %19, %20, %21, %22, %23, %arg6) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<2 x i64>, array<3 x i64>)>, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
    llvm.store %24, %arg0 : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>, !llvm.ptr
    llvm.return
  }
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {llvm.data_layout = ""} {
  llvm.func @malloc(i64) -> !llvm.ptr
  omp.reduction.declare @__scf_reduction : f32 init {
  ^bb0(%arg0: f32):
    %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    omp.yield(%0 : f32)
  } combiner {
  ^bb0(%arg0: f32, %arg1: f32):
    %0 = llvm.fadd %arg0, %arg1  : f32
    omp.yield(%0 : f32)
  } atomic {
  ^bb0(%arg0: !llvm.ptr, %arg1: !llvm.ptr):
    %0 = llvm.load %arg1 : !llvm.ptr -> f32
    %1 = llvm.atomicrmw fadd %arg0, %0 monotonic : !llvm.ptr, f32
    omp.yield
  }
  llvm.func @SpMVMul.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: !llvm.ptr, %arg6: !llvm.ptr, %arg7: i64, %arg8: i64, %arg9: i64, %arg10: !llvm.ptr, %arg11: !llvm.ptr, %arg12: i64, %arg13: i64, %arg14: i64, %arg15: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg16: !llvm.ptr, %arg17: !llvm.ptr, %arg18: i64, %arg19: i64, %arg20: i64, %arg21: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.insertvalue %arg5, %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.insertvalue %arg6, %7[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.insertvalue %arg7, %8[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.insertvalue %arg8, %9[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.insertvalue %arg9, %10[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.insertvalue %arg10, %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.insertvalue %arg11, %13[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.insertvalue %arg12, %14[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.insertvalue %arg13, %15[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.insertvalue %arg14, %16[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = llvm.insertvalue %arg16, %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %20 = llvm.insertvalue %arg17, %19[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.insertvalue %arg18, %20[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %22 = llvm.insertvalue %arg19, %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %23 = llvm.insertvalue %arg20, %22[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %24 = llvm.mlir.constant(0 : index) : i64
    %25 = llvm.mlir.constant(1 : index) : i64
    %26 = llvm.mlir.constant(128 : index) : i64
    %27 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %28 = llvm.mlir.constant(0 : i64) : i64
    %29 = llvm.mlir.constant(128 : i64) : i64
    %30 = llvm.mlir.constant(1 : index) : i64
    %31 = llvm.mlir.constant(128 : index) : i64
    %32 = llvm.mlir.constant(0 : index) : i64
    %33 = llvm.mlir.constant(1 : i64) : i64
    %34 = llvm.mlir.null : !llvm.ptr
    %35 = llvm.getelementptr %34[128] : (!llvm.ptr) -> !llvm.ptr, f32
    %36 = llvm.ptrtoint %35 : !llvm.ptr to i64
    %37 = llvm.call @malloc(%36) : (i64) -> !llvm.ptr
    %38 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %39 = llvm.insertvalue %37, %38[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %40 = llvm.insertvalue %37, %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %41 = llvm.insertvalue %24, %40[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %42 = llvm.insertvalue %26, %41[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %43 = llvm.insertvalue %25, %42[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %44 = llvm.mlir.undef : !llvm.struct<(array<1 x i64>, array<1 x i64>)>
    %45 = llvm.insertvalue %28, %44[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    %46 = llvm.insertvalue %29, %45[0, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    llvm.br ^bb1(%32 : i64)
  ^bb1(%47: i64):  // 2 preds: ^bb0, ^bb2
    %48 = llvm.icmp "slt" %47, %31 : i64
    llvm.cond_br %48, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %49 = llvm.getelementptr %37[%47] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %27, %49 : f32, !llvm.ptr
    %50 = llvm.add %47, %30  : i64
    llvm.br ^bb1(%50 : i64)
  ^bb3:  // pred: ^bb1
    %51 = llvm.insertvalue %29, %46[1, 0] : !llvm.struct<(array<1 x i64>, array<1 x i64>)> 
    omp.parallel   {
      %55 = llvm.alloca %33 x f32 : (i64) -> !llvm.ptr
      omp.wsloop   for  (%arg22) : i64 = (%32) to (%31) step (%30) {
        %56 = llvm.getelementptr %37[%arg22] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %57 = llvm.load %56 : !llvm.ptr -> f32
        %58 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %59 = llvm.getelementptr %58[%arg22] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %60 = llvm.load %59 : !llvm.ptr -> i64
        %61 = llvm.add %arg22, %30  : i64
        %62 = llvm.extractvalue %5[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %63 = llvm.getelementptr %62[%61] : (!llvm.ptr, i64) -> !llvm.ptr, i64
        %64 = llvm.load %63 : !llvm.ptr -> i64
        llvm.store %57, %55 : f32, !llvm.ptr
        omp.parallel   {
          omp.wsloop   reduction(@__scf_reduction -> %55 : !llvm.ptr) for  (%arg23) : i64 = (%60) to (%64) step (%30) {
            %67 = llvm.intr.stacksave : !llvm.ptr
            llvm.br ^bb1
          ^bb1:  // pred: ^bb0
            %68 = llvm.extractvalue %11[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %69 = llvm.getelementptr %68[%arg23] : (!llvm.ptr, i64) -> !llvm.ptr, i64
            %70 = llvm.load %69 : !llvm.ptr -> i64
            %71 = llvm.extractvalue %17[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %72 = llvm.getelementptr %71[%arg23] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %73 = llvm.load %72 : !llvm.ptr -> f32
            %74 = llvm.extractvalue %23[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
            %75 = llvm.getelementptr %74[%70] : (!llvm.ptr, i64) -> !llvm.ptr, f32
            %76 = llvm.load %75 : !llvm.ptr -> f32
            %77 = llvm.fmul %73, %76  : f32
            omp.reduction %77, %55 : f32, !llvm.ptr
            llvm.intr.stackrestore %67 : !llvm.ptr
            llvm.br ^bb2
          ^bb2:  // pred: ^bb1
            omp.yield
          }
          omp.terminator
        }
        %65 = llvm.load %55 : !llvm.ptr -> f32
        %66 = llvm.getelementptr %37[%arg22] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %65, %66 : f32, !llvm.ptr
        omp.yield
      }
      omp.terminator
    }
    %52 = llvm.mlir.undef : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
    %53 = llvm.insertvalue %43, %52[0] : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> 
    %54 = llvm.insertvalue %51, %53[1] : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)> 
    llvm.return %54 : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
  }
  llvm.func @_mlir_ciface_SpMVMul.z.0.main(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.struct<(array<2 x i64>, array<3 x i64>)>, %arg5: !llvm.ptr, %arg6: !llvm.struct<(array<1 x i64>, array<1 x i64>)>) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %3 = llvm.extractvalue %0[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %4 = llvm.extractvalue %0[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %5 = llvm.extractvalue %0[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.extractvalue %6[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %10 = llvm.extractvalue %6[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.extractvalue %6[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %12 = llvm.load %arg3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %13 = llvm.extractvalue %12[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %14 = llvm.extractvalue %12[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %12[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.extractvalue %12[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %17 = llvm.extractvalue %12[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %18 = llvm.load %arg5 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %20 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %21 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %22 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %23 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %24 = llvm.call @SpMVMul.z.0.main(%1, %2, %3, %4, %5, %7, %8, %9, %10, %11, %13, %14, %15, %16, %17, %arg4, %19, %20, %21, %22, %23, %arg6) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<2 x i64>, array<3 x i64>)>, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.struct<(array<1 x i64>, array<1 x i64>)>) -> !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>
    llvm.store %24, %arg0 : !llvm.struct<(struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>, struct<(array<1 x i64>, array<1 x i64>)>)>, !llvm.ptr
    llvm.return
  }
}


